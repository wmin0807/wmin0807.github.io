<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Stylish Jekyll Theme</description>
		<link>/</link>
		<atom:link href="/" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Java学习笔记5</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;泛型&lt;/h4&gt;

&lt;p&gt;所谓泛型就是在定义类、接口、方法时使用类型形参，这个类型形参将在声明变量、创建对象、调用方法时动态的指定。虽然在使用集合类，比如：List、Set等类时，会使用泛型，但是根据定义并不是只针对集合，在定义类的时候就可以指定泛型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Person&amp;lt;T&amp;gt; {
	private T info;
	public Person(T info) {
		this.info = info;
	}
}

Person&amp;lt;String&amp;gt; person = new Person&amp;lt;&amp;gt;(&quot;name&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当创建了带泛型声明的接口、父类的时候，可以为该类接口创建实现类、或从该父类派生出子类。但需要注意的是，当使用这些泛型接口、父类的时候不能再包含类型形参。例如下面是一种错误的描述：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这是一种错误的描述
public class Student extend Person&amp;lt;T&amp;gt; {

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用如下的方式来继承该类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Student extend Person&amp;lt;String&amp;gt; {

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者不指定泛型类型，但是这样会有警告信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Student extend Person {

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有的时候我们在程序中需要使用泛型的父类，这个时候可以使用如下方式实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这种用法有警告信息
public void test(List list) {
	....
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式是正确的，但是使用这种方式可能会引起警告信息，如果采用下面的方式来使用就会引起错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这种用法是错误的
public void test(List&amp;lt;Object&amp;gt; list) {

	...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Java中为了表示泛型类的父类，需要使用类型通配符，就是用？来表示。在上面的例子中可以使用如下的形式来表示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这种表示方式表示所有List类的父类，没有错误也没有警告信息
public void test(List&amp;lt;?&amp;gt; list) {

	...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是这里存在一个问题，如果使用List&amp;lt;?&amp;gt;这种父类，在程序中使用的话还需要强制类型的转换，转换成需要的类型，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void test(List&amp;lt;?&amp;gt; list) {	
	for (Object obj : list) {
		String str = (String)obj;
		...
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种使用方式还需要把对象强制类型转换一下，为了解决这种问题，可以给类型通配符设定一个上线：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void test(List&amp;lt;? extends xxx&amp;gt; list) {
	
	...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式就可以把List中的对象当成xxx类型，直接访问。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在定义泛型类的时候并不希望这个泛型类型是随意添加的，需要指定该类型是某个类型的子类，这个时候需要指定泛型类型的上限。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了解决这个问题需要指定类型形参的上限，例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Question&amp;lt;T extends Person&amp;gt; {

	...
	
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候在使用类型T的时候就必须传入Person的子类。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;泛型方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;泛型方法的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;修饰符 &amp;lt;T, S&amp;gt; 返回值类型 方法名(形参列表) {

...

}

T 和 S 都可以使用这种形式： T extends E，表示E是T的上限。

// 例子如下：
public &amp;lt;T&amp;gt; void fromArrayToCollection(T[] a, Collection&amp;lt;T&amp;gt; c) {
	for (T o : a) {
		c.add(o);
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果可以使用泛型方法，那泛型方法和类型通配符之间存在怎样的区别：在大多数的情况下都可以使用泛型方法来代替类型通配符。通配符就是被设计用来支持灵活的子类化的，但是泛型方法就是允许类型形参被表示方法的一个或者多个参数之间的类型依赖关系，或者方法返回值与参数之间的类型依赖关系。&lt;/p&gt;

</description>
				<pubDate>Fri, 10 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/10/Java%E7%AC%94%E8%AE%B05.html</link>
				<guid isPermaLink="true">/2015/07/10/Java%E7%AC%94%E8%AE%B05.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记4</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;集合&lt;/h4&gt;

&lt;p&gt;Java的集合大致可以分成Set List 和 Map三种体系结构。其中Set代表无序不可重复的集合；List代表有序、重复的集合；而Map代表具有映射关系的集合。&lt;/p&gt;

&lt;p&gt;Java的集合类主要由两个接口派生而来：Collection 和 Map，这两个接口是根接口。Set 和 List两个接口派生于Collection接口。根据Set List 和 Map三种类型的特点，可以得出这样的结论：&lt;/p&gt;

&lt;p&gt;如果访问List集合中的元素，可以直接使用元素的索引；如果访问Map集合需要根据key来访问，访问Set集合则只能根据元素本身。使用比较多的是：
HashSet TreeSet ArrayList ArrayDeque LinkedList HashMap TreeMap&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Set&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Set集合不允许包含相同的元素，如果把两个相同的元素添加到Set集合中，则会添加失败。Set判断两个对象相同不是使用==运算符，而是根据equals方法，也就是说两个对象用equlas方法判断返回true，则可以添加成功。Set的子类包含：HashSet 、 TreeSet 、 EnumSet&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HashSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;HashSet具有以下几个特点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不能保证元素的排列顺序，遍历的元素可能与添加的元素顺序可能不一样。&lt;/li&gt;
  &lt;li&gt;HashSet不是同步的，如果两个以上的线程来修改HashSet则需要手动的保证代码的同步。&lt;/li&gt;
  &lt;li&gt;集合元素可以为null&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HashSet中存入一个元素的时候，系统会根据该元素的hashCode()返回的数字决定存储的位置。如果两个元素根据equals方法返回true，但是hashCode不一样，则HashSet同样会将两个对象添加成功。&lt;/p&gt;

&lt;p&gt;因此HashSet判断两个对象是否相等，需要equals返回true，同时hashCode返回的数值也一样。应该在重写equlas方法的时候，应该也需要重写hashCode方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LinkedHashSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LinkedHashSet 是HashSet的子类，它也是根据hashCode值来决定元素的存储位置，但是它不同的是需要使用链表来维护元素的次序。这样使得元素看起来是以插入的顺序保存的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TreeSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TreeSet是SortedSet接口的实现类，因此它可以确保TreeSet中的元素都是有序的。&lt;/p&gt;

&lt;p&gt;TreeSet集合采用hash算法来决定元素的存储位置，同时TreeSet采用红黑树的数据结构来存储集合元素。TreeSet采用了两种排序方法：自然排序和定制排序。&lt;/p&gt;

&lt;p&gt;自然排序是根据添加到TreeSet中的对象的compareTo(Object obj)来判断。因此在加入到TreeSet中的所有对象都要实现Comparable接口。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Test implements Comparable {
	...
	
	public int comparaTo(Object obj) {
	
		...
	
	}
	...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个Test对象就可以添加到TreeSet集合中。&lt;/p&gt;

&lt;p&gt;定制排序：&lt;/p&gt;

&lt;p&gt;定制排序是创建TreeSet对象的时候指定比较方法，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TreeSet ts = new TreeSet(new Comparator() {
	
	public int compare(Object o1, Object o2) {
		...
	}
	
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样在添加到TreeSet中的对象就不需要自己实现Comparable接口，因为TreeSet已经帮你实现了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ArrayList Vector&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ArrayList &amp;amp;&amp;amp; Vector 都是基于数组实现的List类。他们都是内部封装了一个动态可分配的数组，使用initialCapacity参数来设置该数组的长度，当向ArrayList和Vector中添加的元素超过了数组的长度则initialCapacity将会自动增长。&lt;/p&gt;

&lt;p&gt;ArrayList 与 Vector之间的主要区别是：ArrayList是线程不安全的，当多个线程修改同一个ArrayList集合的时候，需要手动的保证线程的安全。Vector是线程安全的。&lt;/p&gt;

&lt;p&gt;在Arrays中提供了asList(….)方法，把指定的数组转成List集合。&lt;strong&gt;但是需要注意的是转成后的List对象是不可以使用add remove等操作的，只能遍历。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LinkedList&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LinkedList 是List的一个集合，同时还实现了Deque接口，因此它可以被当成双端队列来使用。自然也可以被当成栈来使用。LinkedList内部使用的是链表的形式来保存集合中的元素。因此随机访问的性能不好，但是插入和删除的性能比较好。&lt;/p&gt;

&lt;p&gt;-Xms 是设置JVM的堆内存初始值大小。
-Xmx 是设置JVM的堆内存最大大小。&lt;/p&gt;

&lt;p&gt;总结：如果需要随机访问的性能，则考虑使用ArrayList，如果需要插入、删除的性能，则需要考虑使用LinkedList.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hashtable &amp;amp;&amp;amp; HashMap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;两者之间的区别：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hashtable 是一个线程安全的类，HashMap线程不安全。&lt;/li&gt;
  &lt;li&gt;Hashtable 不允许使用null作为key和value，但是HashMap是可以使用null作为key和value的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此 HashMap中只能有一个key位null，但是可以有无数的value值为null。&lt;/p&gt;

&lt;p&gt;与LinkedHashSet一样，Map中也存在一个LinkedHashMap，用链表来记录map对象的添加顺序。&lt;/p&gt;

&lt;p&gt;SortedMap 与 TreeMap 也是可以排序的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;自然排序： TreeMap 的所有key必须实现Comparable接口，而且所有的key应该是同一个类的对象，否则会抛出异常信息。&lt;/li&gt;
  &lt;li&gt;定制排序： 创建TreeMap对象的时候，传入一个Comparator对象，该对象负责对TreeMap中所有的key进行排序。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Collections&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Java 为 Set List Map 提供了一个工具类： Collections，它提供了大量方法对集合元素进行排序，查询和修改等操作。还提供了将集合元素对象设置为不可变、对集合对象实现同步的控制方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reverse / shuffle / sort / swap / rotate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Collections 提供了多个synchronizedXxx()方法，将集合包装成线程同步的集合。从而解决多线程并发的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Collection c = Collecionts.synchronizedCollection(new ArrayList());
List list = Collecionts.synchronizedList(new ArrayList());
Set set = Collecionts.synchronizedSet(new HashSet());
Map m = Collecionts.synchronizedMap(new HashMap());
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Thu, 09 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/09/Java%E7%AC%94%E8%AE%B04.html</link>
				<guid isPermaLink="true">/2015/07/09/Java%E7%AC%94%E8%AE%B04.html</guid>
			</item>
		
			<item>
				<title>spark-jobserver的安装</title>
				<description>&lt;h4 id=&quot;spark-jobserver&quot;&gt;spark-jobserver的安装&lt;/h4&gt;

&lt;p&gt;spark-jobserver 提供了一个RESTful接口来提交和管理spark的jobs,jars和job contexts。该工程位于：https://github.com/spark-jobserver/spark-jobserver&lt;/p&gt;

&lt;p&gt;特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;针对job 和 contexts的各个方面提供了REST风格的api接口进行管理&lt;/li&gt;
  &lt;li&gt;支持SparkSQL，Hive，Streaming Contexts/jobs 以及定制job contexts!&lt;/li&gt;
  &lt;li&gt;支持压秒级别低延迟的任务通过长期运行的job contexts&lt;/li&gt;
  &lt;li&gt;可以通过结束context来停止运行的作业(job)&lt;/li&gt;
  &lt;li&gt;分割jar上传步骤以提高job的启动&lt;/li&gt;
  &lt;li&gt;异步和同步的job API，其中同步API对低延时作业非常有效&lt;/li&gt;
  &lt;li&gt;支持Standalone Spark和Mesos&lt;/li&gt;
  &lt;li&gt;Job和jar信息通过一个可插拔的DAO接口来持久化&lt;/li&gt;
  &lt;li&gt;命名RDD以缓存，并可以通过该名称获取RDD。这样可以提高作业间RDD的共享和重用&lt;/li&gt;
  &lt;li&gt;支持scala 2.10 和 2.11&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在生产环境上一般直接部署spark-jobserver，部署的过程可以参考官方文档的Deployment这个章节。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;拷贝spark-jobserver 目录下的conf/local.sh.template文件为local.sh，cp conf/local.sh.template conf/local.sh.&lt;/li&gt;
  &lt;li&gt;配置conf/local.sh文件，主要是以下三个参数SPARK_VERISON / SPARK_HOME/ DRIVER_MEMORY&lt;/li&gt;
  &lt;li&gt;然后执行bin/server_deploy.sh config/local开始部署配置环境，这个时候可能会提醒你找不到local.sh文件，你可以根据错误提示信息把local.sh文件拷贝到相应的路径下。&lt;/li&gt;
  &lt;li&gt;在执行完server_deploy.sh文件后，可以执行server_start.sh文件进行启动jobserver。可能在启动的过程提示log4j找不到，这个时候需要把conf目录下的log4j文件拷贝到指定的位置。同时如果提示：spark-jobserver-master/bin/spark-job-server.jar does not exist，则个时候需要找一下当前路径下的spark-job-server.jar文件在哪个路径下，然后把该文件拷贝到bin目录下。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;启动没有报错的情况下，可以用ps -ef&lt;/td&gt;
          &lt;td&gt;grep job来查看一下当前是否存在jobserver的进程。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;正确启动后就可以通过浏览器访问该主机的8090端口，例如：192.168.1.100:8090。&lt;/li&gt;
  &lt;li&gt;可以在该主机上执行：curl –data-binary @job-server-tests/target/job-server-tests-$VER.jar localhost:8090/jars/test，把测试jar包放到jobserver上。&lt;/li&gt;
  &lt;li&gt;然后可以在Spark上执行任务：curl -d “input.string = a b c a b see” ‘localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample’&lt;/li&gt;
  &lt;li&gt;执行完后可以查看执行的结果：curl localhost:8090/jobs/5453779a-f004-45fc-a11d-a39dae0f9bf4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;jobserver的安装就完成。具体如何让编写的spark应用程序可以被jobserver调用，这需要创建一个jobserver的工程。&lt;/p&gt;
</description>
				<pubDate>Tue, 07 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/07/spark-jobserver%E7%9A%84%E5%AE%89%E8%A3%85.html</link>
				<guid isPermaLink="true">/2015/07/07/spark-jobserver%E7%9A%84%E5%AE%89%E8%A3%85.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记3</title>
				<description>&lt;h4 id=&quot;string--stringbuffer--stringbuilder---&quot;&gt;String / StringBuffer / StringBuilder / 时间 的使用&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;String StringBuffer StringBuilder&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;String 类是不可变类，一旦创建以后，包含在这个对象中的字符序列是不可以改变的，直至这个对象被销毁。&lt;/p&gt;

&lt;p&gt;StringBuffer 和 StringBuilder则是代表一个字符序列可变的类，他们提供了append() insert() reverse() setChartAt() setLength() 方法来改变这个字符序列。&lt;/p&gt;

&lt;p&gt;StringBuffer 和 StringBuilder两个类的用法基本相似，但是StringBuffer是线程安全的，而StringBuilder则没有实现线程安全的功能。所以StringBuilder的性能要高一些。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Date类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Java 提供Date类来处理日期与时间。由于Date存在的时间比较悠久导致很多构造方法都已经过时，不再推荐使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Date()
Date(long date)
boolean after(Date when)
.........before.........
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于Date的过时，现在大部分都是使用Calendar工具类。Calendar是一个抽象类，不能直接实例化。但是它提供了几个静态的getInstance()方法来获取Calendar对象。&lt;/p&gt;

&lt;p&gt;Calendar 与 Date之间的转换。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Calendar cl = Calendar.getInstance();
Date date = cl.getTime();

/**
 * 
 * Calendar 提供了add、set方法，field的类型有：Calendar.YEAR Calendar.MONTH....
 * 但是需要注意的是： Calendar.MONTH字段代表的月份，月份的起始值不是1，而是0。
 * @author wangmin
 *
 */
public class CalendarTest {
	public static void main(String[] args) {
		Calendar c = Calendar.getInstance();
		System.out.println(c.get(Calendar.YEAR));
		System.out.println(c.get(Calendar.MONDAY));
		System.out.println(c.get(Calendar.MONTH));
	}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Mon, 06 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/06/Java%E7%AC%94%E8%AE%B03.html</link>
				<guid isPermaLink="true">/2015/07/06/Java%E7%AC%94%E8%AE%B03.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记2</title>
				<description>&lt;h4 id=&quot;singleton-----&quot;&gt;Singleton / 不可变类 / 缓存不可变类 的实现&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Singleton（单例类）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;单例类用的地方很多，如果一个类始终只能创建一个实例，则这个类被称为单例类。&lt;/p&gt;

&lt;p&gt;以下程序是创建一个单例类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SingletonTest {
	public static void main(String[] args) {
		Singleton s1 = Singleton.getInstance();
		Singleton s2 = Singleton.getInstance();
		System.out.println(s1 == s2);
		s1.setName(&quot;Singleton1&quot;);
		System.out.println(&quot;s1 = &quot; + s1.getName());
		System.out.println(&quot;s2 = &quot; + s2.getName());
		s2.setName(&quot;Singleton2&quot;);
		System.out.println(&quot;s1 = &quot; + s1.getName());
		System.out.println(&quot;s2 = &quot; + s2.getName());
	}
}
class Singleton {
	private String name;
	private static Singleton singleton = new Singleton(&quot;default&quot;);
	private Singleton(String name) {
		this.name = name;
	}
	public static Singleton getInstance() {
		return singleton;
	}
	public void setName(String name) {
		this.name = name;
	}
	public String getName() {
		return this.name;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;不可变类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;不可变类就是在创建该类的实例后不能更改该类的属性。Java提供的8个包装类和java.lang.String都是不可变类。&lt;/p&gt;

&lt;p&gt;创建不可变类的几个规则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用private 和 final 修饰符修饰属性；&lt;/li&gt;
  &lt;li&gt;提供带参数的构建起，用于初始化属性值；&lt;/li&gt;
  &lt;li&gt;只提供getter方法；&lt;/li&gt;
  &lt;li&gt;重写hashCode 和 equlas方法；&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Person 类是一个不可变类
 * @author wangmin
 *
 */
public class Person {
	private final String name;
	private final int age;
	public Person() {
		this.name = &quot;&quot;;
		this.age = 0;
	}
	public Person(String name, int age) {
		this.name = name;
		this.age = age;
	}
	public String getName() {
		return this.name;
	}
	public int getAge() {
		return this.age;
	}
	public boolean equlas (Object obj) {
		if (this == obj) {
			return true;
		}
		if (obj != null &amp;amp;&amp;amp; obj.getClass() == this.getClass()) {
			Person p = (Person)obj;
			if (this.getName().equals(p.getName()) &amp;amp;&amp;amp; this.age == p.getAge()) {
				return true;
			}
		}
		return false;
	}
	public int hasCode() {
		return name.hashCode() + age;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;缓存不可变类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果程序经常需要使用相同的不可变类实例，则应该考虑缓存这种不可变类。下面是简单的使用数组的方式实现一个不可变类。&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 使用数组的方式来缓解Person
 * 1. 定义数组大小，并且使用private static 修饰
 * 2. 定义valueOf方法获取对象，但是其中存在以下几种情况：
 * 	2.1 循环遍历整个缓存数组的大小，然后比较，如果已经被缓存则直接取出。（注意数组元素可能为null）
 *  2.2 由2.1的结果可知，如果数组中没有则新创建。
 *  2.3 首次判断pos是否已满，如果已满则将新的对象创建到数组0的位置，并且将pos设置为0，并返回。
 *  2.4 如果没有满则根据pos的位置创建，并返回。
 * @author wangmin
 *
 */
public class CachePerson {
	private static int MAX_SIZE = 10;
	private static CachePerson[] cp = new CachePerson[MAX_SIZE];
	private String name;
	private static int pos = 0;
	
	public CachePerson() {
		
	}
	public CachePerson(String name) {
		this.name = name;
	}
	public String getName() {
		return this.name;
	}
	public static CachePerson valueOf(String name) {
		for (int i = 0; i &amp;lt; MAX_SIZE; i++) {
			if (cp[i] != null &amp;amp;&amp;amp; cp[i].getName().equals(name)) {
				return cp[i];
			}
		}
		if (pos == MAX_SIZE) {
			cp[0] = new CachePerson(name);
			pos = 1;
		} else {
			cp[pos++] = new CachePerson(name);
		}
		return cp[pos-1];
	}
	public boolean equlas (Object obj) {
		if (this == obj) {
			return true;
		}
		if (obj != null &amp;amp;&amp;amp; obj.getClass() == this.getClass()) {
			CachePerson cp = (CachePerson)obj;
			if (cp.getName().equals(this.getName())) {
				return true;
			}
		}
		return false;
	}
	public int hasCode() {
		return name.hashCode();
	}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Sun, 05 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/05/Java%E7%AC%94%E8%AE%B02.html</link>
				<guid isPermaLink="true">/2015/07/05/Java%E7%AC%94%E8%AE%B02.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记1</title>
				<description>&lt;h4 id=&quot;tostring----equals-&quot;&gt;toString / == / equals 的使用&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;toString的使用&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Person p = new Person();
System.out.println(p);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行上面的程序得到的结果是：Person@f72645&lt;/p&gt;

&lt;p&gt;其实输出这个结果就是调用了对象的toString()方法，也就是其结果和调用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;System.out.println(p.toString())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;toString是一个自我描述的方法，一般都是在直接打印该对象的时候，输出该对象的描述信息。要实现这个自我描述方法，需要自己重写toString方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Person {
	private String name;
	private double weight;
	....
	publci String toString() {
		return &quot;name: &quot; + name + &quot;, weight: &quot; + weight;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的Person类就是重写了toString方法，在打印该对象的时候，就是打印：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: xxxx, weight: xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;equlas方法和==的区别&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;判断两个对象是否相等可以使用equals方法，另一种是使用==判断。&lt;/p&gt;

&lt;p&gt;使用 == 判断两个对象是否相等的条件是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果两个对象都是基本类型，那么他们的值相等就返回true&lt;/li&gt;
  &lt;li&gt;如果非基本类型，则说明他们的引用需要指向同一个对象，这个时候才返回true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int i = 100;
float f = 100.0f;
i == f 返回true

String str1 = new String(&quot;hello&quot;);
String str2 = new String(&quot;hello&quot;);
str1 == str2 这个将输出false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而equlas是Object提供的一个方法，使用这个方法来判断两个对象是否相等，和使用 == 来判断是一样的效果，但是equlas方法是可以被子自定义的类重写的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Person {

	...
	
	public boolean equlas(Object obj) {
		... 
		return true;
	}
} 在使用equlas比较的时候就会调用自定义的equlas方法，提供一个标准的equlas方法重写的模板：

public boolean equlas(Object obj) {

	if (this == obj) {
		return true;
	}
	if (obj != null  &amp;amp;&amp;amp; obj.getClass() == Person.class) {
		Person pobj = (Person)obj;
		if (...) {
			return true;
		}
	}
	return false;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Object 默认提供的equlas只是比较对象的地址，即Object类的equlas方法比较的结果与 == 运算符比较的结果是完全相同。因此，在实际运用中需要重写equals，这时候可以根据自己的条件来决定如何判断相等。&lt;/p&gt;

</description>
				<pubDate>Thu, 02 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/02/Java%E7%AC%94%E8%AE%B01.html</link>
				<guid isPermaLink="true">/2015/07/02/Java%E7%AC%94%E8%AE%B01.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记4</title>
				<description>&lt;h4 id=&quot;standalone&quot;&gt;任务的提交以及Standalone集群模式的部署&lt;/h4&gt;

&lt;h4 id=&quot;spark-submit&quot;&gt;spark-submit&lt;/h4&gt;

&lt;p&gt;首先需要打包代码，如果你的代码需要依赖其他的包环境则需要单独的打包这些依赖，应为cluster会将所有依赖的jar包分发到各个节点上进行使用。推荐的方法是将依赖包和程序都统一的打成一个包，这样就可以直接使用spark-submit方法来运行，具体的pom.xml配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependencies&amp;gt;
	&amp;lt;dependency&amp;gt; &amp;lt;!-- Spark dependency --&amp;gt;
		&amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;5.1.13&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&amp;lt;build&amp;gt;
	&amp;lt;plugins&amp;gt;
		&amp;lt;plugin&amp;gt;
			&amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;maven-compiler-plugin&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;2.3.2&amp;lt;/version&amp;gt;
			&amp;lt;configuration&amp;gt;
				&amp;lt;!-- 使用1.7 jdk进行编译 --&amp;gt;
				&amp;lt;source&amp;gt;1.7&amp;lt;/source&amp;gt;
				&amp;lt;target&amp;gt;1.7&amp;lt;/target&amp;gt;
			&amp;lt;/configuration&amp;gt;
		&amp;lt;/plugin&amp;gt;
		&amp;lt;plugin&amp;gt;
			&amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;2.5.5&amp;lt;/version&amp;gt;
			&amp;lt;configuration&amp;gt;
				&amp;lt;descriptorRefs&amp;gt;
					&amp;lt;descriptorRef&amp;gt;jar-with-dependencies&amp;lt;/descriptorRef&amp;gt;
				&amp;lt;/descriptorRefs&amp;gt;
			&amp;lt;/configuration&amp;gt;
			&amp;lt;executions&amp;gt;
				&amp;lt;execution&amp;gt;
					&amp;lt;id&amp;gt;make-assembly&amp;lt;/id&amp;gt;
					&amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
					&amp;lt;goals&amp;gt;
						&amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;
					&amp;lt;/goals&amp;gt;
				&amp;lt;/execution&amp;gt;
			&amp;lt;/executions&amp;gt;
		&amp;lt;/plugin&amp;gt;
	&amp;lt;/plugins&amp;gt;
&amp;lt;/build&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spark &amp;amp;&amp;amp; hadoop 的scope值都设置为provided，然后打包的过程是把需要依赖的第三方东西打成一个统一的jar包去运行，使用的是maven-compiler-plugin插件。&lt;/p&gt;

&lt;p&gt;在服务器上提交的命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./bin/spark-submit \
  --class &amp;lt;main-class&amp;gt;
  --master &amp;lt;master-url&amp;gt; \
  --deploy-mode &amp;lt;deploy-mode&amp;gt; \
  --conf &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; \
  ... # other options
  &amp;lt;application-jar&amp;gt; \
  [application-arguments]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spark-submit 可以加载一个配置文件，默认是加载在conf/spark-defaults.conf&lt;/p&gt;

&lt;h4 id=&quot;spark-standalone-mode&quot;&gt;Spark Standalone Mode&lt;/h4&gt;

&lt;p&gt;除了运行在Mesos和YARN集群之外，spark也提供了简单的独立部署模式。可以通过手动的启动master和worker，也可以通过spark提供的启动脚本来启动。独立部署也可以通过运行在一个机器上，进行测试。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;为了安装你需要放置一个编译好的spark版本到每个机器上。&lt;/li&gt;
  &lt;li&gt;启动集群有两种方式，一种是手动启动，另一种是通过启动脚本启动。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;手动启动spark集群&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;启动一个独立的master可以使用如下的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./sbin/start-master.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一旦启动可以通过访问：http://localhost:8080端口访问master，打出的spark://HOST:PORT 的URL，work节点可以使用这个链接连到master上。&lt;/p&gt;

&lt;p&gt;可以使用如下的命令来使worker节点连接到master上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./sbin/start-slave.sh &amp;lt;worker#&amp;gt; &amp;lt;master-spark-URL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;worker在加入到master后可以访问master的http://localhost:8080，可以清洗的看到被加入的worker节点的信息。&lt;/p&gt;

&lt;p&gt;在启动master和worker的时候可以带上参数进行设置，参数的列表如下:其中比较重要的是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;-c CORES， 这个是指定多少个cpu分配给spark使用，默认是全部cpu&lt;/li&gt;
  &lt;li&gt;-m MEM，这个是指定多少的内存分配给spark使用，默认是全部的内存的减去1g的操作系统内存全部分配给spark使用。一般的格式是1000M or 2G&lt;/li&gt;
  &lt;li&gt;-d DIR, 这个指定spark任务的日志输出目录。&lt;/li&gt;
  &lt;li&gt;–properties-file FILE 指定spark指定加载的配置文件的路径默认是： conf/spark-defaults.conf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;脚本方式部署&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过spark的部署脚本部署首先需要在spark的主目录下创建一个conf/slaves的文件，这个文件中每一行代表一个worker的hostname.需要注意的是，master访问worker节点是通过SSH访问的，所以需要master通过ssh无密码的登录到worker，否则需要设置一个 SPARK_SSH_FOREGROUND的环境变量，这个变量的值就是每个worker的密码&lt;/p&gt;

&lt;p&gt;然后可以通过spark安装目录下的sbin/….sh文件进行启动，
如果要启动和停止master和slave可以使用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sbin/start-all.sh&lt;/li&gt;
  &lt;li&gt;sbin/stop-all.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;注意的是这些脚本必须是在master机器上执行&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;同时你可以通过配置集群的 conf/spark-env.sh文件来进一步配置集群的环境。但是这也文件需要通过拷贝conf/spark-env.sh.template文件来创建，并且需要把这个文件拷贝到所有的worker节点上。&lt;/p&gt;

&lt;p&gt;其中： SPARK_MASTER_OPTS &amp;amp;&amp;amp; SPARK_WORKER_OPTS 两个配置项比较复杂。&lt;/p&gt;

&lt;p&gt;通过在SparkContext构造器中传入spark://IP:PORT这个来启用这个集群。同时可以在交互式的方式启动脚本中使用：./bin/spark-shell –master spark://IP:PORT 来启动集群执行。&lt;/p&gt;

&lt;p&gt;独立部署模式的集群现在只是简单的支持FIFO调度。
为了允许多个并发用户，可以通过SparkConf设置每个应用程序需要的资源的最大数。默认情况下，它会请求使用集群的全部的核，而这只是同时运行一个应用程序才回有意义。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val conf = new SparkConf()
             .setMaster(...)
             .setAppName(...)
             .set(&quot;spark.cores.max&quot;, &quot;10&quot;)
val sc = new SparkContext(conf)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了可以在程序中指定你也可以在spark-env.sh中设置默认的值，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=&amp;lt;value&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;spark的高可用设置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;spark的高可用设置有两种，一种是通过Zookeeper来实现，另一种是通过本地文件系统来实现。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;使用ZooKeeper备份master，利用zookeeper提供的领导选举和状态保存，你可以让更多的master连接到zookeepre实例。一个将会被选举为leader其他的则会保存备份他的状态。如果master死掉，zookeeper可以选举一个新的leader，整个过程需要1到2分钟的时间，但是这个过程只会对新的任务调度有影响。为了使用这种方式需要的配置项为：SPARK_DAEMON_JAVA_OPTS，这个配置项有三个配置信息：spark.deploy.recoveryMode/spark.deploy.zookeeper.url/spark.deploy.zookeeper.dir&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用本地文件系统来恢复该节点。为了使用这种方式需要的配置项为：SPARK_DAEMON_JAVA_OPTS，这个配置项有两个配置信息：spark.deploy.recoveryMode、spark.deploy.recoveryDirectory&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Mon, 29 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/29/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04.html</link>
				<guid isPermaLink="true">/2015/06/29/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记3</title>
				<description>&lt;h4 id=&quot;hdfsmysql&quot;&gt;读取HDFS中的数据，并简单分析，最后结果写入mysql数据库中。&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;首先建立工程，pom文件中引入以下几个依赖，&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt; &amp;lt;!-- Spark dependency --&amp;gt;
	&amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
	&amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;5.1.13&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
	&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
	&amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
	&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先需要引入spark的包，这里使用的是spark1.4的包。由于需要读取HDFS中的数据，所以需要hadoop-client文件，这个Hadoop的环境是2.6.0的环境。最后需要把结果写入mysql中，需要mysql的驱动文件，所以需要加入mysql的依赖，最后加入单元测试的包。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建文件ReadHDFSErrorToMysql.java&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在main函数中首先创建JavaSparkcontext对象。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;SparkConf conf = new SparkConf().setAppName(&quot;FindError&quot;);
JavaSparkContext sc = new JavaSparkContext(conf);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;找到指定目录下的所有文件路径，因为只有找到这个路径才能加载相应的文件。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 
 * 列出指定目录中的文件，这里的文件是不包括子目录的。
 * @param pathOfDirectory
 * 	目录路径
 * @return
 * @throws IOException 
 */
public static String[] findFilePathFromDir(String dst) throws IOException {
	Set&amp;lt;String&amp;gt; filePathSet = new HashSet&amp;lt;String&amp;gt;();
	String[] result = null;
	Configuration conf = new Configuration();
	FileSystem fs = FileSystem.get(URI.create(dst), conf);
	FileStatus fileList[] = fs.listStatus(new Path(dst));
	int size = fileList.length;
	for (int i = 0; i &amp;lt; size; i++) {
		filePathSet.add(fileList[i].getPath().toString());
	}
	if (filePathSet.size() &amp;gt; 0) {
		result = new String[filePathSet.size()];
		int i = 0;
		for (String str : filePathSet) {
			result[i++] = str;
		}
	}
	fs.close();
	return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;依次遍历文件路径并为每个文件创建一个新的RDD然后计算出这个文件中包涵ERROR字符串的行数。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;Map&amp;lt;String, Long&amp;gt; result = new HashMap&amp;lt;String, Long&amp;gt;();
if (filePaths != null) {
	for (String path : filePaths) {
		result.put(path, sc.textFile(path).filter(new Function&amp;lt;String, Boolean&amp;gt;() {

			public Boolean call(String line) throws Exception {
				return line.contains(&quot;ERROR&quot;);
			}
			
		}).count());
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;将results中的数据写入mysql中&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 将结果写入mysql中
 * @param result
 * @throws Exception 
 */
public static void wirteResultToMysql(Map&amp;lt;String, Long&amp;gt; result) throws Exception {
    String DBDRIVER = &quot;com.mysql.jdbc.Driver&quot;;  
    //连接地址是由各个数据库生产商单独提供的，所以需要单独记住  
    String DBURL = &quot;jdbc:mysql://ip:3306/test&quot;;  
    //连接数据库的用户名  
    String DBUSER = &quot;root&quot;;  
    //连接数据库的密码  
    String DBPASS = &quot;root&quot;;
    Connection con = null; //表示数据库的连接对象  
    PreparedStatement pstmt = null; //表示数据库更新操作  
    String sql = &quot;insert into aaa values(?,?)&quot;;  
    Class.forName(DBDRIVER); //1、使用CLASS 类加载驱动程序  
    con = DriverManager.getConnection(DBURL,DBUSER,DBPASS); //2、连接数据库  
    pstmt = con.prepareStatement(sql); //使用预处理的方式创建对象  
    if (result != null) {
    	for (String str : result.keySet()) {
    		pstmt.setString(1, str);
    		pstmt.setLong(2, result.get(str));
    		pstmt.addBatch();
    	}
    }
    //pstmt.executeUpdate(); //执行SQL 语句，更新数据库  
    pstmt.executeBatch();
    pstmt.close();  
    con.close(); // 4、关闭数据库  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section&quot;&gt;总结一下：&lt;/h4&gt;

&lt;p&gt;虽然整个过程比较简单，但通过实际的编码可以更加熟悉spark的api和功能。&lt;/p&gt;
</description>
				<pubDate>Sun, 28 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03.html</link>
				<guid isPermaLink="true">/2015/06/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03.html</guid>
			</item>
		
			<item>
				<title>Linux一些常用的命令</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;查看内存的使用情况&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;free  [-m/-g] 使用free -g 可以以G为单位显示当前系统内存的使用状况。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;cpumemswap-&quot;&gt;查看 CPU/Mem/Swap 的使用情况&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;top -M
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section-1&quot;&gt;查看磁盘的使用情况以及文件系统被挂载的位置&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;df -lh, 也可以使用 fdisk -l 来查看详细的磁盘分区情况
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section-2&quot;&gt;解压文件到指定的路径&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;tar -xzvf ....tar.gz -C 解压文件存放的位置
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;scp&quot;&gt;scp命令拷贝文件&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;在linux下一般用scp这个命令来通过ssh传输文件。

1、从服务器上下载文件
scp username@servername:/path/filename /var/www/local_dir（本地目录）

 例如scp root@192.168.0.101:/var/www/test.txt  把192.168.0.101上的/var/www/test.txt 的文件下载到/var/www/local_dir（本地目录）

2、上传本地文件到服务器
scp /path/filename username@servername:/path   

例如scp /var/www/test.php  root@192.168.0.101:/var/www/  把本机/var/www/目录下的test.php文件上传到192.168.0.101这台服务器上的/var/www/目录中

3、从服务器下载整个目录
scp -r username@servername:/var/www/remote_dir/（远程目录） /var/www/local_dir（本地目录）

例如:scp -r root@192.168.0.101:/var/www/test  /var/www/  

4、上传目录到服务器
scp  -r local_dir username@servername:remote_dir
例如：scp -r test  root@192.168.0.101:/var/www/   把当前目录下的test目录上传到服务器的/var/www/ 目录
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Fri, 26 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/26/Linux%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4.html</link>
				<guid isPermaLink="true">/2015/06/26/Linux%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4.html</guid>
			</item>
		
			<item>
				<title>ssh_exchange_identification</title>
				<description>&lt;p&gt;在使用ssh user@ip连接远程主机的时候出现这个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_exchange_identification: Connection closed by remote host 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;针对这个问题做一下简单的总结：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首先可以使用telent yourhost 22查看一下端口是否能通。&lt;/li&gt;
  &lt;li&gt;首先怀疑是该机器out of memory，因为ssh 到一台机器的时候，这台机器会fork进程来完成这个操作，如果是系统的进程数太多超过限制，或者内存不足无法满足fork子进程操作所需要的资源，操作系统会主动断开链接。如果需要更多的信息可以使用ssh -v …进行链接操作查看更多的错误信息。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;有可能是yourhost 机器下的/etc/hosts.allow文件不允许你的ip地址进行ssh登录。这个时候需要修改这个文件，具体的修改可以参考这个链接地址：&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;http://www.rjkfw.com/s_596.html&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 23 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/23/Ssh_exchange_identification.html</link>
				<guid isPermaLink="true">/2015/06/23/Ssh_exchange_identification.html</guid>
			</item>
		
	</channel>
</rss>
