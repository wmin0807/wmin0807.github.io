<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Stylish Jekyll Theme</description>
		<link>/</link>
		<atom:link href="/" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Java学习笔记3</title>
				<description>&lt;h4 id=&quot;string--stringbuffer--stringbuilder---&quot;&gt;String / StringBuffer / StringBuilder / 时间 的使用&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;String StringBuffer StringBuilder&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;String 类是不可变类，一旦创建以后，包含在这个对象中的字符序列是不可以改变的，直至这个对象被销毁。&lt;/p&gt;

&lt;p&gt;StringBuffer 和 StringBuilder则是代表一个字符序列可变的类，他们提供了append() insert() reverse() setChartAt() setLength() 方法来改变这个字符序列。&lt;/p&gt;

&lt;p&gt;StringBuffer 和 StringBuilder两个类的用法基本相似，但是StringBuffer是线程安全的，而StringBuilder则没有实现线程安全的功能。所以StringBuilder的性能要高一些。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Date类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Java 提供Date类来处理日期与时间。由于Date存在的时间比较悠久导致很多构造方法都已经过时，不再推荐使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Date()
Date(long date)
boolean after(Date when)
.........before.........
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于Date的过时，现在大部分都是使用Calendar工具类。Calendar是一个抽象类，不能直接实例化。但是它提供了几个静态的getInstance()方法来获取Calendar对象。&lt;/p&gt;

&lt;p&gt;Calendar 与 Date之间的转换。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Calendar cl = Calendar.getInstance();
Date date = cl.getTime();

/**
 * 
 * Calendar 提供了add、set方法，field的类型有：Calendar.YEAR Calendar.MONTH....
 * 但是需要注意的是： Calendar.MONTH字段代表的月份，月份的起始值不是1，而是0。
 * @author wangmin
 *
 */
public class CalendarTest {
	public static void main(String[] args) {
		Calendar c = Calendar.getInstance();
		System.out.println(c.get(Calendar.YEAR));
		System.out.println(c.get(Calendar.MONDAY));
		System.out.println(c.get(Calendar.MONTH));
	}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Mon, 06 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/06/Java%E7%AC%94%E8%AE%B03.html</link>
				<guid isPermaLink="true">/2015/07/06/Java%E7%AC%94%E8%AE%B03.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记2</title>
				<description>&lt;h4 id=&quot;singleton-----&quot;&gt;Singleton / 不可变类 / 缓存不可变类 的实现&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Singleton（单例类）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;单例类用的地方很多，如果一个类始终只能创建一个实例，则这个类被称为单例类。&lt;/p&gt;

&lt;p&gt;以下程序是创建一个单例类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SingletonTest {
	public static void main(String[] args) {
		Singleton s1 = Singleton.getInstance();
		Singleton s2 = Singleton.getInstance();
		System.out.println(s1 == s2);
		s1.setName(&quot;Singleton1&quot;);
		System.out.println(&quot;s1 = &quot; + s1.getName());
		System.out.println(&quot;s2 = &quot; + s2.getName());
		s2.setName(&quot;Singleton2&quot;);
		System.out.println(&quot;s1 = &quot; + s1.getName());
		System.out.println(&quot;s2 = &quot; + s2.getName());
	}
}
class Singleton {
	private String name;
	private static Singleton singleton = new Singleton(&quot;default&quot;);
	private Singleton(String name) {
		this.name = name;
	}
	public static Singleton getInstance() {
		return singleton;
	}
	public void setName(String name) {
		this.name = name;
	}
	public String getName() {
		return this.name;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;不可变类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;不可变类就是在创建该类的实例后不能更改该类的属性。Java提供的8个包装类和java.lang.String都是不可变类。&lt;/p&gt;

&lt;p&gt;创建不可变类的几个规则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用private 和 final 修饰符修饰属性；&lt;/li&gt;
  &lt;li&gt;提供带参数的构建起，用于初始化属性值；&lt;/li&gt;
  &lt;li&gt;只提供getter方法；&lt;/li&gt;
  &lt;li&gt;重写hashCode 和 equlas方法；&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Person 类是一个不可变类
 * @author wangmin
 *
 */
public class Person {
	private final String name;
	private final int age;
	public Person() {
		this.name = &quot;&quot;;
		this.age = 0;
	}
	public Person(String name, int age) {
		this.name = name;
		this.age = age;
	}
	public String getName() {
		return this.name;
	}
	public int getAge() {
		return this.age;
	}
	public boolean equlas (Object obj) {
		if (this == obj) {
			return true;
		}
		if (obj != null &amp;amp;&amp;amp; obj.getClass() == this.getClass()) {
			Person p = (Person)obj;
			if (this.getName().equals(p.getName()) &amp;amp;&amp;amp; this.age == p.getAge()) {
				return true;
			}
		}
		return false;
	}
	public int hasCode() {
		return name.hashCode() + age;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;缓存不可变类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果程序经常需要使用相同的不可变类实例，则应该考虑缓存这种不可变类。下面是简单的使用数组的方式实现一个不可变类。&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 使用数组的方式来缓解Person
 * 1. 定义数组大小，并且使用private static 修饰
 * 2. 定义valueOf方法获取对象，但是其中存在以下几种情况：
 * 	2.1 循环遍历整个缓存数组的大小，然后比较，如果已经被缓存则直接取出。（注意数组元素可能为null）
 *  2.2 由2.1的结果可知，如果数组中没有则新创建。
 *  2.3 首次判断pos是否已满，如果已满则将新的对象创建到数组0的位置，并且将pos设置为0，并返回。
 *  2.4 如果没有满则根据pos的位置创建，并返回。
 * @author wangmin
 *
 */
public class CachePerson {
	private static int MAX_SIZE = 10;
	private static CachePerson[] cp = new CachePerson[MAX_SIZE];
	private String name;
	private static int pos = 0;
	
	public CachePerson() {
		
	}
	public CachePerson(String name) {
		this.name = name;
	}
	public String getName() {
		return this.name;
	}
	public static CachePerson valueOf(String name) {
		for (int i = 0; i &amp;lt; MAX_SIZE; i++) {
			if (cp[i] != null &amp;amp;&amp;amp; cp[i].getName().equals(name)) {
				return cp[i];
			}
		}
		if (pos == MAX_SIZE) {
			cp[0] = new CachePerson(name);
			pos = 1;
		} else {
			cp[pos++] = new CachePerson(name);
		}
		return cp[pos-1];
	}
	public boolean equlas (Object obj) {
		if (this == obj) {
			return true;
		}
		if (obj != null &amp;amp;&amp;amp; obj.getClass() == this.getClass()) {
			CachePerson cp = (CachePerson)obj;
			if (cp.getName().equals(this.getName())) {
				return true;
			}
		}
		return false;
	}
	public int hasCode() {
		return name.hashCode();
	}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Sun, 05 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/05/Java%E7%AC%94%E8%AE%B02.html</link>
				<guid isPermaLink="true">/2015/07/05/Java%E7%AC%94%E8%AE%B02.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记1</title>
				<description>&lt;h4 id=&quot;tostring----equals-&quot;&gt;toString / == / equals 的使用&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;toString的使用&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Person p = new Person();
System.out.println(p);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行上面的程序得到的结果是：Person@f72645&lt;/p&gt;

&lt;p&gt;其实输出这个结果就是调用了对象的toString()方法，也就是其结果和调用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;System.out.println(p.toString())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;toString是一个自我描述的方法，一般都是在直接打印该对象的时候，输出该对象的描述信息。要实现这个自我描述方法，需要自己重写toString方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Person {
	private String name;
	private double weight;
	....
	publci String toString() {
		return &quot;name: &quot; + name + &quot;, weight: &quot; + weight;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的Person类就是重写了toString方法，在打印该对象的时候，就是打印：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: xxxx, weight: xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;equlas方法和==的区别&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;判断两个对象是否相等可以使用equals方法，另一种是使用==判断。&lt;/p&gt;

&lt;p&gt;使用 == 判断两个对象是否相等的条件是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果两个对象都是基本类型，那么他们的值相等就返回true&lt;/li&gt;
  &lt;li&gt;如果非基本类型，则说明他们的引用需要指向同一个对象，这个时候才返回true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int i = 100;
float f = 100.0f;
i == f 返回true

String str1 = new String(&quot;hello&quot;);
String str2 = new String(&quot;hello&quot;);
str1 == str2 这个将输出false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而equlas是Object提供的一个方法，使用这个方法来判断两个对象是否相等，和使用 == 来判断是一样的效果，但是equlas方法是可以被子自定义的类重写的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Person {

	...
	
	public boolean equlas(Object obj) {
		... 
		return true;
	}
} 在使用equlas比较的时候就会调用自定义的equlas方法，提供一个标准的equlas方法重写的模板：

public boolean equlas(Object obj) {

	if (this == obj) {
		return true;
	}
	if (obj != null  &amp;amp;&amp;amp; obj.getClass() == Person.class) {
		Person pobj = (Person)obj;
		if (...) {
			return true;
		}
	}
	return false;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Object 默认提供的equlas只是比较对象的地址，即Object类的equlas方法比较的结果与 == 运算符比较的结果是完全相同。因此，在实际运用中需要重写equals，这时候可以根据自己的条件来决定如何判断相等。&lt;/p&gt;

</description>
				<pubDate>Thu, 02 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/02/Java%E7%AC%94%E8%AE%B01.html</link>
				<guid isPermaLink="true">/2015/07/02/Java%E7%AC%94%E8%AE%B01.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记4</title>
				<description>&lt;h4 id=&quot;standalone&quot;&gt;任务的提交以及Standalone集群模式的部署&lt;/h4&gt;

&lt;h4 id=&quot;spark-submit&quot;&gt;spark-submit&lt;/h4&gt;

&lt;p&gt;首先需要打包代码，如果你的代码需要依赖其他的包环境则需要单独的打包这些依赖，应为cluster会将所有依赖的jar包分发到各个节点上进行使用。推荐的方法是将依赖包和程序都统一的打成一个包，这样就可以直接使用spark-submit方法来运行，具体的pom.xml配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependencies&amp;gt;
	&amp;lt;dependency&amp;gt; &amp;lt;!-- Spark dependency --&amp;gt;
		&amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;5.1.13&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&amp;lt;build&amp;gt;
	&amp;lt;plugins&amp;gt;
		&amp;lt;plugin&amp;gt;
			&amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;maven-compiler-plugin&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;2.3.2&amp;lt;/version&amp;gt;
			&amp;lt;configuration&amp;gt;
				&amp;lt;!-- 使用1.7 jdk进行编译 --&amp;gt;
				&amp;lt;source&amp;gt;1.7&amp;lt;/source&amp;gt;
				&amp;lt;target&amp;gt;1.7&amp;lt;/target&amp;gt;
			&amp;lt;/configuration&amp;gt;
		&amp;lt;/plugin&amp;gt;
		&amp;lt;plugin&amp;gt;
			&amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;2.5.5&amp;lt;/version&amp;gt;
			&amp;lt;configuration&amp;gt;
				&amp;lt;descriptorRefs&amp;gt;
					&amp;lt;descriptorRef&amp;gt;jar-with-dependencies&amp;lt;/descriptorRef&amp;gt;
				&amp;lt;/descriptorRefs&amp;gt;
			&amp;lt;/configuration&amp;gt;
			&amp;lt;executions&amp;gt;
				&amp;lt;execution&amp;gt;
					&amp;lt;id&amp;gt;make-assembly&amp;lt;/id&amp;gt;
					&amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
					&amp;lt;goals&amp;gt;
						&amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;
					&amp;lt;/goals&amp;gt;
				&amp;lt;/execution&amp;gt;
			&amp;lt;/executions&amp;gt;
		&amp;lt;/plugin&amp;gt;
	&amp;lt;/plugins&amp;gt;
&amp;lt;/build&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spark &amp;amp;&amp;amp; hadoop 的scope值都设置为provided，然后打包的过程是把需要依赖的第三方东西打成一个统一的jar包去运行，使用的是maven-compiler-plugin插件。&lt;/p&gt;

&lt;p&gt;在服务器上提交的命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./bin/spark-submit \
  --class &amp;lt;main-class&amp;gt;
  --master &amp;lt;master-url&amp;gt; \
  --deploy-mode &amp;lt;deploy-mode&amp;gt; \
  --conf &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; \
  ... # other options
  &amp;lt;application-jar&amp;gt; \
  [application-arguments]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spark-submit 可以加载一个配置文件，默认是加载在conf/spark-defaults.conf&lt;/p&gt;

&lt;h4 id=&quot;spark-standalone-mode&quot;&gt;Spark Standalone Mode&lt;/h4&gt;

&lt;p&gt;除了运行在Mesos和YARN集群之外，spark也提供了简单的独立部署模式。可以通过手动的启动master和worker，也可以通过spark提供的启动脚本来启动。独立部署也可以通过运行在一个机器上，进行测试。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;为了安装你需要放置一个编译好的spark版本到每个机器上。&lt;/li&gt;
  &lt;li&gt;启动集群有两种方式，一种是手动启动，另一种是通过启动脚本启动。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;手动启动spark集群&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;启动一个独立的master可以使用如下的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./sbin/start-master.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一旦启动可以通过访问：http://localhost:8080端口访问master，打出的spark://HOST:PORT 的URL，work节点可以使用这个链接连到master上。&lt;/p&gt;

&lt;p&gt;可以使用如下的命令来使worker节点连接到master上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./sbin/start-slave.sh &amp;lt;worker#&amp;gt; &amp;lt;master-spark-URL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;worker在加入到master后可以访问master的http://localhost:8080，可以清洗的看到被加入的worker节点的信息。&lt;/p&gt;

&lt;p&gt;在启动master和worker的时候可以带上参数进行设置，参数的列表如下:其中比较重要的是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;-c CORES， 这个是指定多少个cpu分配给spark使用，默认是全部cpu&lt;/li&gt;
  &lt;li&gt;-m MEM，这个是指定多少的内存分配给spark使用，默认是全部的内存的减去1g的操作系统内存全部分配给spark使用。一般的格式是1000M or 2G&lt;/li&gt;
  &lt;li&gt;-d DIR, 这个指定spark任务的日志输出目录。&lt;/li&gt;
  &lt;li&gt;–properties-file FILE 指定spark指定加载的配置文件的路径默认是： conf/spark-defaults.conf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;脚本方式部署&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过spark的部署脚本部署首先需要在spark的主目录下创建一个conf/slaves的文件，这个文件中每一行代表一个worker的hostname.需要注意的是，master访问worker节点是通过SSH访问的，所以需要master通过ssh无密码的登录到worker，否则需要设置一个 SPARK_SSH_FOREGROUND的环境变量，这个变量的值就是每个worker的密码&lt;/p&gt;

&lt;p&gt;然后可以通过spark安装目录下的sbin/….sh文件进行启动，
如果要启动和停止master和slave可以使用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sbin/start-all.sh&lt;/li&gt;
  &lt;li&gt;sbin/stop-all.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;注意的是这些脚本必须是在master机器上执行&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;同时你可以通过配置集群的 conf/spark-env.sh文件来进一步配置集群的环境。但是这也文件需要通过拷贝conf/spark-env.sh.template文件来创建，并且需要把这个文件拷贝到所有的worker节点上。&lt;/p&gt;

&lt;p&gt;其中： SPARK_MASTER_OPTS &amp;amp;&amp;amp; SPARK_WORKER_OPTS 两个配置项比较复杂。&lt;/p&gt;

&lt;p&gt;通过在SparkContext构造器中传入spark://IP:PORT这个来启用这个集群。同时可以在交互式的方式启动脚本中使用：./bin/spark-shell –master spark://IP:PORT 来启动集群执行。&lt;/p&gt;

&lt;p&gt;独立部署模式的集群现在只是简单的支持FIFO调度。
为了允许多个并发用户，可以通过SparkConf设置每个应用程序需要的资源的最大数。默认情况下，它会请求使用集群的全部的核，而这只是同时运行一个应用程序才回有意义。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val conf = new SparkConf()
             .setMaster(...)
             .setAppName(...)
             .set(&quot;spark.cores.max&quot;, &quot;10&quot;)
val sc = new SparkContext(conf)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了可以在程序中指定你也可以在spark-env.sh中设置默认的值，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=&amp;lt;value&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;spark的高可用设置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;spark的高可用设置有两种，一种是通过Zookeeper来实现，另一种是通过本地文件系统来实现。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;使用ZooKeeper备份master，利用zookeeper提供的领导选举和状态保存，你可以让更多的master连接到zookeepre实例。一个将会被选举为leader其他的则会保存备份他的状态。如果master死掉，zookeeper可以选举一个新的leader，整个过程需要1到2分钟的时间，但是这个过程只会对新的任务调度有影响。为了使用这种方式需要的配置项为：SPARK_DAEMON_JAVA_OPTS，这个配置项有三个配置信息：spark.deploy.recoveryMode/spark.deploy.zookeeper.url/spark.deploy.zookeeper.dir&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用本地文件系统来恢复该节点。为了使用这种方式需要的配置项为：SPARK_DAEMON_JAVA_OPTS，这个配置项有两个配置信息：spark.deploy.recoveryMode、spark.deploy.recoveryDirectory&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Mon, 29 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/29/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04.html</link>
				<guid isPermaLink="true">/2015/06/29/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记3</title>
				<description>&lt;h4 id=&quot;hdfsmysql&quot;&gt;读取HDFS中的数据，并简单分析，最后结果写入mysql数据库中。&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;首先建立工程，pom文件中引入以下几个依赖，&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt; &amp;lt;!-- Spark dependency --&amp;gt;
	&amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
	&amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;5.1.13&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
	&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
	&amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
	&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先需要引入spark的包，这里使用的是spark1.4的包。由于需要读取HDFS中的数据，所以需要hadoop-client文件，这个Hadoop的环境是2.6.0的环境。最后需要把结果写入mysql中，需要mysql的驱动文件，所以需要加入mysql的依赖，最后加入单元测试的包。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建文件ReadHDFSErrorToMysql.java&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在main函数中首先创建JavaSparkcontext对象。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;SparkConf conf = new SparkConf().setAppName(&quot;FindError&quot;);
JavaSparkContext sc = new JavaSparkContext(conf);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;找到指定目录下的所有文件路径，因为只有找到这个路径才能加载相应的文件。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 
 * 列出指定目录中的文件，这里的文件是不包括子目录的。
 * @param pathOfDirectory
 * 	目录路径
 * @return
 * @throws IOException 
 */
public static String[] findFilePathFromDir(String dst) throws IOException {
	Set&amp;lt;String&amp;gt; filePathSet = new HashSet&amp;lt;String&amp;gt;();
	String[] result = null;
	Configuration conf = new Configuration();
	FileSystem fs = FileSystem.get(URI.create(dst), conf);
	FileStatus fileList[] = fs.listStatus(new Path(dst));
	int size = fileList.length;
	for (int i = 0; i &amp;lt; size; i++) {
		filePathSet.add(fileList[i].getPath().toString());
	}
	if (filePathSet.size() &amp;gt; 0) {
		result = new String[filePathSet.size()];
		int i = 0;
		for (String str : filePathSet) {
			result[i++] = str;
		}
	}
	fs.close();
	return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;依次遍历文件路径并为每个文件创建一个新的RDD然后计算出这个文件中包涵ERROR字符串的行数。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;Map&amp;lt;String, Long&amp;gt; result = new HashMap&amp;lt;String, Long&amp;gt;();
if (filePaths != null) {
	for (String path : filePaths) {
		result.put(path, sc.textFile(path).filter(new Function&amp;lt;String, Boolean&amp;gt;() {

			public Boolean call(String line) throws Exception {
				return line.contains(&quot;ERROR&quot;);
			}
			
		}).count());
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;将results中的数据写入mysql中&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 将结果写入mysql中
 * @param result
 * @throws Exception 
 */
public static void wirteResultToMysql(Map&amp;lt;String, Long&amp;gt; result) throws Exception {
    String DBDRIVER = &quot;com.mysql.jdbc.Driver&quot;;  
    //连接地址是由各个数据库生产商单独提供的，所以需要单独记住  
    String DBURL = &quot;jdbc:mysql://ip:3306/test&quot;;  
    //连接数据库的用户名  
    String DBUSER = &quot;root&quot;;  
    //连接数据库的密码  
    String DBPASS = &quot;root&quot;;
    Connection con = null; //表示数据库的连接对象  
    PreparedStatement pstmt = null; //表示数据库更新操作  
    String sql = &quot;insert into aaa values(?,?)&quot;;  
    Class.forName(DBDRIVER); //1、使用CLASS 类加载驱动程序  
    con = DriverManager.getConnection(DBURL,DBUSER,DBPASS); //2、连接数据库  
    pstmt = con.prepareStatement(sql); //使用预处理的方式创建对象  
    if (result != null) {
    	for (String str : result.keySet()) {
    		pstmt.setString(1, str);
    		pstmt.setLong(2, result.get(str));
    		pstmt.addBatch();
    	}
    }
    //pstmt.executeUpdate(); //执行SQL 语句，更新数据库  
    pstmt.executeBatch();
    pstmt.close();  
    con.close(); // 4、关闭数据库  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section&quot;&gt;总结一下：&lt;/h4&gt;

&lt;p&gt;虽然整个过程比较简单，但通过实际的编码可以更加熟悉spark的api和功能。&lt;/p&gt;
</description>
				<pubDate>Sun, 28 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03.html</link>
				<guid isPermaLink="true">/2015/06/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03.html</guid>
			</item>
		
			<item>
				<title>Linux一些常用的命令</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;查看内存的使用情况&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;free  [-m/-g] 使用free -g 可以以G为单位显示当前系统内存的使用状况。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;cpumemswap-&quot;&gt;查看 CPU/Mem/Swap 的使用情况&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;top -M
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section-1&quot;&gt;查看磁盘的使用情况以及文件系统被挂载的位置&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;df -lh, 也可以使用 fdisk -l 来查看详细的磁盘分区情况
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section-2&quot;&gt;解压文件到指定的路径&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;tar -xzvf ....tar.gz -C 解压文件存放的位置
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;scp&quot;&gt;scp命令拷贝文件&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;在linux下一般用scp这个命令来通过ssh传输文件。

1、从服务器上下载文件
scp username@servername:/path/filename /var/www/local_dir（本地目录）

 例如scp root@192.168.0.101:/var/www/test.txt  把192.168.0.101上的/var/www/test.txt 的文件下载到/var/www/local_dir（本地目录）

2、上传本地文件到服务器
scp /path/filename username@servername:/path   

例如scp /var/www/test.php  root@192.168.0.101:/var/www/  把本机/var/www/目录下的test.php文件上传到192.168.0.101这台服务器上的/var/www/目录中

3、从服务器下载整个目录
scp -r username@servername:/var/www/remote_dir/（远程目录） /var/www/local_dir（本地目录）

例如:scp -r root@192.168.0.101:/var/www/test  /var/www/  

4、上传目录到服务器
scp  -r local_dir username@servername:remote_dir
例如：scp -r test  root@192.168.0.101:/var/www/   把当前目录下的test目录上传到服务器的/var/www/ 目录
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Fri, 26 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/26/Linux%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4.html</link>
				<guid isPermaLink="true">/2015/06/26/Linux%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4.html</guid>
			</item>
		
			<item>
				<title>ssh_exchange_identification</title>
				<description>&lt;p&gt;在使用ssh user@ip连接远程主机的时候出现这个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_exchange_identification: Connection closed by remote host 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;针对这个问题做一下简单的总结：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首先可以使用telent yourhost 22查看一下端口是否能通。&lt;/li&gt;
  &lt;li&gt;首先怀疑是该机器out of memory，因为ssh 到一台机器的时候，这台机器会fork进程来完成这个操作，如果是系统的进程数太多超过限制，或者内存不足无法满足fork子进程操作所需要的资源，操作系统会主动断开链接。如果需要更多的信息可以使用ssh -v …进行链接操作查看更多的错误信息。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;有可能是yourhost 机器下的/etc/hosts.allow文件不允许你的ip地址进行ssh登录。这个时候需要修改这个文件，具体的修改可以参考这个链接地址：&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;http://www.rjkfw.com/s_596.html&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 23 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/23/Ssh_exchange_identification.html</link>
				<guid isPermaLink="true">/2015/06/23/Ssh_exchange_identification.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记2</title>
				<description>&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;每一个spark程序都是有一个驱动程序组成，并且通过main函数运行。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;spark有两个重要的抽象：&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;RDD，分布式弹性数据集，他是一个跨越多个节点的分布式集合。&lt;/li&gt;
      &lt;li&gt;另一个抽象是共享变量。spark支持两种类型的共享变量：一个是广播（broadcast variables）他可以缓存一个值在集群的各个节点。另一个是累加器（accumulators）他只能执行累加的操作，比如可以做计数器和求和。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Initializing Spark&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;运行spark程序，需要在程序中创建一个JavaSparkContext对象，但是在创建JavaSparkContext对象的时候需要指定SparkConf对象，这个对象是配置一些重要的参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;SparkConf conf = new SparkConf.setAppName(appName).setMaster(url);
JavaSparkContext sc = new JavaSparkContext(conf); ---
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;appName就是这个任务的名字，url代表spark集群的Url地址。这个地址不要写死，因为这样可以通过命令行来传入需要执行的spark集群，方便调试与线上集群运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Resilient Distributed Datasets (RDDs)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Spark的核心就是围绕着RDD，它是一个自动容错的分布式数据集合。他有两种方式创建，第一种就是在驱动程序中对一个集合进行并行化。第二种是来源于一个外部的存储系统。比如：共享系统、HDFS、HBase或者任何提供任何Hadoop 输入格式的数据源。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;第一种：Parallelized Collections 创建这个集合需要调用那个JavaSparkContext的parallelize方法来初始化一个已经存在的集合。&lt;/li&gt;
    &lt;/ul&gt;

    &lt;hr /&gt;

    &lt;pre&gt;&lt;code&gt;  List&amp;lt;Integer&amp;gt; data = Arrays.asList(1,2,3,4,5);
  JavaRDD&amp;lt;Iteger&amp;gt; distData = sc.parallelize(data);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;这就创建了一个并行的集合，在这个集合上可以执行	distData.reduce((a, b) -&amp;gt; a + b)&lt;/p&gt;

    &lt;p&gt;在并行数组中一个很重要的参数是partitions，它来描述数组被切割的数据集数量。Spark会在每一个partitions上运行任务，这个partitions会被spark自动设置，一般都是集群中每个CPU上运行2-4partitions，但是也可以自己设置，可以通过parallelize （e.g. sc.parallelize(data, 10)），在有些地方把partitions成为 slices。&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;External Datasets&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;JavaRDD&lt;string&gt; distFile = sc.textFile(&quot;data.txt&quot;); &lt;/string&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;textFile也可以设置partitions参数，一般都是一个block一个partitions，但是也可以自己设置，自己设置必须要不能少于block的数量。&lt;/p&gt;

    &lt;p&gt;针对Hadoop的其他输入格式，你能用这个JavaSparkContext.hadoopRDD方法，你需要设置JobConf和输入格式的类。也可以使用JavaSparkContext.newAPIHadoopRDD针对输入格式是基于“new”的MapReduceAPI&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RDD Operations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RDD的操作可以分成两类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. transformations 
2. actions
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;map就是一个transformation操作，他会针对数据集的每一个元素执行一个函数。然后返回一个新的RDD来替换掉原先的RDD。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;reduce就是一个action操作，他能够聚合所有的元素通过执行一个函数，然后返回给驱动程序一个最终的值。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;所有的transformations都是lazy操作，也就是说并不会马上计算结果，要等到执行一个action操作并且返回一个值给驱动程序的时候才会执行transformations操作。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;可以调用RDD的 persist (or cache) 方法来持久化RDD，这样的话在下次使用RDD的时候就会非常快。同时也支持持久化到磁盘，或者复制到多个节点。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;直接使用 rdd.foreach(println) 在local模式下是可行的，但是在cluster模式下是不行的，必须要执行collect()方法，将所有的数据拉取到本地，然后执行foreach()操作。如果是数据量比较小的话可以使用take方法，rdd.take(100).foreach(println)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 22 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/22/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.html</link>
				<guid isPermaLink="true">/2015/06/22/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.html</guid>
			</item>
		
			<item>
				<title>Junit使用笔记</title>
				<description>&lt;p&gt;Junit的所有测试方法都是以@Test修饰，以public void 开头。如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Test
public void testAdd() {
	assertEquals(0, new Calculate().add(0, 1));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@BeforeClass &amp;amp;&amp;amp; @AfterClass 都是只会执行一次，@BeforeClass是在类加载的时候执行，@AfterClass 是整个类结束的时候被执行，整个方法是一个静态方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@BeforeClass
public static void setUpBeforeClass() throws Exception {
	System.out.println(&quot;before class&quot;);
}

@AfterClass
public static void tearDownAfterClass() throws Exception {
	System.out.println(&quot;after class&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Before &amp;amp;&amp;amp; @After两个方法是在每个测试方法执行的执行都会被执行，@Before是在方法执行前执行，@After是在方法执行结束后执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Before
public void setUp() throws Exception {
	System.out.println(&quot;before&quot;);
}

@After
public void tearDown() throws Exception {
	System.out.println(&quot;after&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Ignore 所修饰的测试方法会被测试运行器忽略，例如以下的test1方法就会被测试运行器忽略执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Ignore
@Test
public void test1() {
	System.out.println(&quot;test1&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Test(timeout=毫秒），用来指定时间上限，如果这个测试方法的执行时间超过了这个时间值则测试失败。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 会执行失败，因为sleep的时间长于设定的timeout时间
@Test(timeout=1000)
public void test() {
	try {
		Thread.sleep(2000);
	} catch (InterruptedException e) {
		e.printStackTrace();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Test(expected=异常类)，用expected来指定应该抛出的异常，如果在执行过程中没有抛出异常或者抛出的异常不是指定的异常，则测试失败。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这个测试案例会执行成功，因为指定的异常就是程序要抛出的异常
@Test(expected=IndexOutOfBoundsException.class) 
public void outOfBounds() 
{
    new ArrayList&amp;lt;Object&amp;gt;().get(1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试套件就是组织所要测试的类一起运行，如果单个类单独的运行是比较麻烦的，可以使用测试套件一起运行这些测试类。需要注意的是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;测试套件的类是不包含其他任何方法&lt;/li&gt;
  &lt;li&gt;同时要更改测试运行器为Suite.class&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将要测试的类作为数组传入到SuiteClasses({})中&lt;/p&gt;

    &lt;hr /&gt;

    &lt;pre&gt;&lt;code&gt;  // 更改测试运行器以及将要测试的类放入SuiteClasses中
  @RunWith(Suite.class)
  @SuiteClasses({ AppTest.class, CalculateTest.class, JunitFlowTest.class })
  public class AllTests {
      // 没有测试方法
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 22 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/22/Junit%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</link>
				<guid isPermaLink="true">/2015/06/22/Junit%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记1</title>
				<description>&lt;p&gt;&lt;strong&gt;Apache Spark is a fast and general-purpose cluster computing system.&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;spark 提供了 Java  Scala Python and 的API。
在examples/src/main目录下有Java和Scala例子， 用 bin/run-example 运行。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过运行： ./bin/spark-shell –master local[2] 来进行交互式的操作，这是学习sprak最好的方式&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;从1.4起spark也提供了R的api，./bin/sparkR –master local[2]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;spark 提供了Scala 和 Python的交互式分析shell来学习api
./bin/spark-shell&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RDD是分布式弹性数据集，可以理解为就是一个分布式的集合，这个集合的创建可以通过Hadoop 的 InputFormats，或者通过其他RDD转换而来。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RDD有动作，他能够返回值，以及转换操作，他会返回一个指针指向新的RDD，通过RDD的filter操作返回一个新的RDD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RDD有很复杂的操作，他可以直接调用Scala Java的库方法，使用的时候需要使用：import java.lang.Math来引入。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Caching&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;spark 支持推送一个数据集到一个集群的缓存中，尤其是那种需要经常重复读取的数据。eg: linesWithSpark.cache()&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Self-contained Applications（独立的应用程序）&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建Maven项目。&lt;/li&gt;
  &lt;li&gt;pom.xml 文件如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependencies&amp;gt;
   &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
      &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
    &amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;创建一个简单的spark程序：&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;public class SimpleApp {
	public static void main(String[] args) {
		// 文件路径
		String logFile = &quot;/home/wm/apps/spark-1.4.0-bin-hadoop2.6/README.md&quot;;
		SparkConf conf = new SparkConf().setAppName(&quot;Simple Application&quot;);
		JavaSparkContext sc = new JavaSparkContext(conf);
		JavaRDD&amp;lt;String&amp;gt; logData = sc.textFile(logFile).cache();
		@SuppressWarnings(&quot;serial&quot;)
		long numAs = logData.filter(new Function&amp;lt;String, Boolean&amp;gt;() {
			public Boolean call(String s) throws Exception {
				return s.contains(&quot;a&quot;);
			}
			
		}).count();
		@SuppressWarnings(&quot;serial&quot;)
		long numBs = logData.filter(new Function&amp;lt;String, Boolean&amp;gt;() {

			public Boolean call(String s) throws Exception {
				return s.contains(&quot;b&quot;);
			}
			
		}).count();
		System.out.println(&quot;Lines with a: &quot; + numAs + &quot;, lines with b: &quot; + numBs);
		sc.close();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;mvn pckage&lt;/li&gt;
  &lt;li&gt;将target目录下的sparkdemo2-0.0.1-SNAPSHOT.jar文件放在spark的安装目录下。&lt;/li&gt;
  &lt;li&gt;在spark安装目录下执行jar包中的程序，但是要指定执行的class文件，这个class文件需要全路径。例如：&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;./bin/spark-submit --class &quot;com.wm.test.sparkdemo&quot; --master local[4] sparkdemo-0.0.1-SNAPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;

</description>
				<pubDate>Sun, 21 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/21/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.html</link>
				<guid isPermaLink="true">/2015/06/21/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.html</guid>
			</item>
		
	</channel>
</rss>
