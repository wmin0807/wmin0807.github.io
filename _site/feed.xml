<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Stylish Jekyll Theme</description>
		<link>/</link>
		<atom:link href="/" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Hive学习笔记1</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;第一部分&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;什么是Hive：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hive是基于Hadoop之上的数据仓库，数据存放在HDFS上，它同样可以通过ETL来进行数据的抽取、转换和加载。同时Hive可以自己开发Mapreduce程序来完成本身不能提供的数据处理操作。Hive本身就是一个SQL的解析引擎，他将SQL	语句转成Mapreduce任务在hadoop之上执行。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;什么是数据仓库：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;数据仓库是一个面向主题的，集成的，不可更新的，随时间不变化的数据集合，它用于支持企业或组织的决策分析处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;针对数据仓库的概念的解释：首先数据仓库中的数据是面向主题的，也就是这些数据的都是为了描述同一类事情，同时它的数据主要用于查询操作，不会对数据仓库中的数据进行删除和更新操作。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;OLTP:&lt;/strong&gt; 联机事务处理（面向的是事务，需要实时的更新操作，银行转账）&lt;br /&gt;
&lt;strong&gt;OLAP:&lt;/strong&gt; 联机分析处理（面向历史数据，进行数据的分析与挖掘，主要面向查询，不会做更新和插入数据，推荐系统）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意:&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;在搭建数据仓库的过程最常用的两种模型就是：星型模型和雪花模型，雪花模型是在星型模型上发展出来的。什么是星型模型，比如一个商品的推荐系统，主题应该是商品，但是围绕商品的有客户信息、厂家信息、促销信息等很多信息，这样就组成了一个星型模型。但是客户信息中也存在客户的家庭的信息、地址信息等。这样再关联的话就是一个雪花模型了。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;section-1&quot;&gt;第二部分&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Hive的体系结构：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hive将元数据存储在数据库中（metastore），这个数据库支持mysql、derby等数据库中。Hive默认是存储在derby数据库中。&lt;/p&gt;

&lt;p&gt;Hive的元数据有哪些？ 包括表的名字、表的列和分区及其属性，表的属性包括是否为外部表等，表的数据所在目录等。&lt;/p&gt;

&lt;p&gt;首先Hive是基于Hadoop的，所以hive的数据会使用HDFS进行保存，同时hive的查询操作也是转化成hadoop的MapReduce操作，所以在hive中会存在一个Hive Driver：包括编译器、解析器和优化器。&lt;/p&gt;

&lt;p&gt;在Hive的驱动之前有访问接口、jdbc以及WebConsole等方式进行操作。当然hive的元信息是存放在关系型数据库中的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HQL的执行过程：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后的Mapreduce调用执行。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;section-2&quot;&gt;第三部分&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Hive的安装：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hive的官网地址： hive.apache.org
apache 的历史工程发布页面在： archive.apache.org，在这个页面下可以找到hive工程。这里使用的是0.13版本。&lt;/p&gt;

&lt;p&gt;Hive的安装之前需要先将hadoop安装好，hive有三种安装方式：嵌入模式、本地模式、远程模式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;嵌入模式：&lt;/strong&gt;&lt;br /&gt;
	Hive将元信息存储在Hive自带的Derby数据库中。但是这种操作方式存在一些缺陷：
	1. 只允许创建一个连接，也就是只允许一个用户操作hive
	2. 多用于Demo&lt;/p&gt;

&lt;p&gt;在安装hive之前需要先安装hadoop，然后把hive的压缩包解压，在bin目录下执行./hive进入hive的启动脚本，这种方式就是使用的嵌入式模式启动hive，会在当前目录下生成一个metastore_db的目录，这个就是元信息目录。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本地模式：&lt;/strong&gt;&lt;br /&gt;
	Hive将元信息存储在mysql数据库中，mysql数据库与hive运行在同一台物理机上。这种方式可以允许多个用户操作hive，可以用于开发和测试。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;远程模式：（推荐使用这种方式）&lt;/strong&gt;&lt;br /&gt;
	hive将元信息存储在mysql数据库中，mysql数据库与hive运行在不同的物理机上。&lt;/p&gt;

&lt;p&gt;元信息存储在远程的mysql中。进入远程mysql中，mysql -u … -p … , 进入后使用create database hive创建一个hive数据库来保存元数据。&lt;/p&gt;

&lt;p&gt;在hive解压后，把mysql的驱动jar包上传到hive的lib目录中。这样hive才能操作mysql数据库。然后需要更改hive的配置文件。在conf中创建一个hive-site.xml文件，文件的内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;jdbc:mysql://ip:3306/hive&amp;lt;/value&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当配置文件设置完后以后，就可以启动hive了。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;section-3&quot;&gt;第四部分&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Hive的管理&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用hive的脚本直接进入hive，或者 hive –service cli&lt;/p&gt;

&lt;p&gt;cli常用的命令：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;清屏：ctrl + l 或者 !clear;&lt;/li&gt;
  &lt;li&gt;查看数据仓库中的表： show tables;&lt;/li&gt;
  &lt;li&gt;查看数据仓库中的函数：show functions;&lt;/li&gt;
  &lt;li&gt;查看表结构： desc 表名；&lt;/li&gt;
  &lt;li&gt;查看hdfs上的文件列表： dfs -ls /user 查看hadoop上/user上的文件&lt;/li&gt;
  &lt;li&gt;执行linux命令： !命令&lt;/li&gt;
  &lt;li&gt;执行HQL语句：select * from test;当执行这个语句的时候是不会开启一个MapReduce任务的，因为这个是获取全部的数据，获取全部的数据只需要把所有的数据读取出来就可以了，并不需要启动一个任务。但是在执行select name from test;查询某一个字段的信息的时候就需要启动一个MapReduce任务了。&lt;/li&gt;
  &lt;li&gt;可以执行一个sql脚本：source /root/test.sql 这个语句就是启动一个执行sql脚本。这种方式就是和mysql执行外部的信息一样。&lt;/li&gt;
  &lt;li&gt;hive -S 进入hive启动任务不会产生调试信息，直接产生MapReduce的结果。&lt;/li&gt;
  &lt;li&gt;hive -e 执行sql语句。hive -e ‘show tables’;这样的执行就直接在linux的命令行操作就行，并不需要进入hive的交互式中执行。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Web界面方式：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;启动方式： #hive –service hwi &amp;amp;
在0.13.0中并没有包含web管理的war包，需要自己编译。&lt;/p&gt;

&lt;p&gt;下载hive源码包，并且解压源码包，然后进入源码路径下的hwi目录，使用：jar cvfM0 hive-hwi-0.13.0.war -C web/ . 这样就会打成一个war包，把这个war包拷贝到hive的lib目录下，同时需要修改hive-site.xml 配置文件，这个修改可以在wiki上看到。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.hwi.listen.host&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;0.0.0.0&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;This is the host address the Hive Web Interface will listen on&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;
 
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.hwi.listen.port&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;9999&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;This is the port the Hive Web Interface will listen on&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;
 
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.hwi.war.file&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;lib/hive-hwi-0.13.0.war&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;This is the WAR file with the jsp content for Hive Web Interface&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就可以使用hive –service hwi启动web服务了，但是在访问这个web应用的时候，浏览器还是报出了500的错误。这个问题需要拷贝jdk的tools.jar 拷贝到hive的lib目录下。&lt;/p&gt;

&lt;p&gt;这个时候就可以打开这个web界面了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;hive的远程服务&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;启动hive的远程服务的命令如下： hive –service hiveserver &amp;amp;
如果要使用jdbc连接hive进行操作，这个时候就需要开启hive的远程服务。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;section-4&quot;&gt;第五部分&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Hive的数据类型&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基本数据类型：&lt;/p&gt;

    &lt;p&gt;tinyint/smallint/int/bigint 整数类型&lt;br /&gt;
  float/double 浮点类型&lt;br /&gt;
  boolean 布尔类型&lt;br /&gt;
  string/varchar/char 字符串类型&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;复杂数据类型：&lt;/p&gt;

    &lt;p&gt;array:数组类型，由一系列相同的数据类型的元素组成&lt;br /&gt;
  map:集合类型，包含key-&amp;gt;value键值对，可以通过key来访问元素。&lt;br /&gt;
  struct:结构类型，可以包含不同数据类型的元素，这些元素可以通
  过“点语法”的方式来得到所需要的元素。&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;create table student (
age int,
name string,
grade array&amp;lt;float&amp;gt;);

插入的时候就是： {1, wangmin, [10,20,30]};

create table student1 (
sid int,
sname string,
grade map&amp;lt;string, float&amp;gt;);

插入数据的时候： {1,wangmin,&amp;lt;&#39;大学语文&#39;, 85&amp;gt;}

create table studnet3(
sid int,
sname string,
grades array&amp;lt;map&amp;lt;string, float&amp;gt;&amp;gt;);

插入数据的时候：{1, wangmin, [&amp;lt;&#39;大学语文‘， 12&amp;gt;,&amp;lt;&#39;大学英语’，23&amp;gt;]}

create table student4(
sid int,
info struct&amp;lt;nname:string, age:int, sex:string&amp;gt;);

插入数据的时候：{1, {&#39;wangmin&#39;, 23, ‘男’}}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;时间类型:&lt;/p&gt;

    &lt;p&gt;Date: 日期（年月日）&lt;br /&gt;
  Timestamp: 是unix的一个时间偏移量&lt;br /&gt;
  select unix_timestamp(); 查看系统的时间偏移量&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;section-5&quot;&gt;第六部分&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Hive的数据存储&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;基于HDFS，没有专门的数据存储格式&lt;/p&gt;

&lt;p&gt;Hive的数据模型：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;表可以分成以下几种：&lt;/li&gt;
  &lt;li&gt;Table 内部表&lt;/li&gt;
  &lt;li&gt;Partition 分区表&lt;/li&gt;
  &lt;li&gt;External Table 外部表&lt;/li&gt;
  &lt;li&gt;Bucket Table 桶表&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;create table t1
(tid int, tname string, age int);

create table t2
(tid int, tname string age int);
localtion &#39;/mytable/hive/t2&#39;;

create table t3
（tid int, tname string, age int)
row format delimited fields terminated by &#39;,&#39;;

create table t4
as
select * from sample_date;

create table t5
row format delimited fields terminated by &#39;,&#39;
as
select * from sample_data;

分区表：

create table partition_table
(sid int, sname string)
partitioned by (gender string)
row format delimited fields terminated by &#39;,&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建这张表的时候就是以gender进行分区
分区表能够加快查询效率&lt;/p&gt;
</description>
				<pubDate>Sun, 02 Aug 2015 00:00:00 +0800</pubDate>
				<link>/2015/08/02/Hive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.html</link>
				<guid isPermaLink="true">/2015/08/02/Hive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记7</title>
				<description>&lt;h4 id=&quot;jdbc&quot;&gt;JDBC&lt;/h4&gt;

&lt;p&gt;JDBC的全称为：Java Database Connectivity，即java数据库连接。
使用JDBC开发程序，不仅可以跨数据库，而且还是跨平台的，也就是说使用JDBC开发的程序可以在windows和unix上使用，同时可以针对mysql和oracle等其他数据库，而程序不需要做任何的修改。&lt;/p&gt;

&lt;p&gt;mysql的连接命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql -p 密码 -u 用户名 -h 主机名
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这也可以连接远程主机。
mysql 数据库通常支持以下两种存储机制：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MyISAM：这是mysql早期默认的存储机制，对事务的支持不够好。&lt;/li&gt;
  &lt;li&gt;InnoDB：InnoDB提供了事务安全的存储机制。InnoDB通过建立行级锁来保证事物完整性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DDL 数据定义语言：create alter drop truncate
DML 数据操作语言：insert update delete
DCL 数据控制语言：grant revoke&lt;/p&gt;

&lt;p&gt;create table/index/view/function/procedure/trigger…&lt;/p&gt;

&lt;p&gt;创建表的过程：&lt;/p&gt;

&lt;p&gt;create table [模式名.]表名
{
	columnName1 datatype [default expr],
	…
}&lt;/p&gt;

&lt;p&gt;数据库字段类型：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;tinyint/smallint/mediumint/int/bigint&lt;/li&gt;
  &lt;li&gt;float/double&lt;/li&gt;
  &lt;li&gt;decimal(dec)&lt;/li&gt;
  &lt;li&gt;date&lt;/li&gt;
  &lt;li&gt;time&lt;/li&gt;
  &lt;li&gt;datetime&lt;/li&gt;
  &lt;li&gt;timstamp(时间戳）&lt;/li&gt;
  &lt;li&gt;year&lt;/li&gt;
  &lt;li&gt;char&lt;/li&gt;
  &lt;li&gt;varchar&lt;/li&gt;
  &lt;li&gt;binary&lt;/li&gt;
  &lt;li&gt;varbinary&lt;/li&gt;
  &lt;li&gt;tinyblob/blob/mediumblob/longblob&lt;/li&gt;
  &lt;li&gt;tinytext/text/mediumtext/longtext&lt;/li&gt;
  &lt;li&gt;enum(‘value1’,’value2’…) 其中的一个&lt;/li&gt;
  &lt;li&gt;set(‘value1’,’value2’…) 其中的几个&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;JDBC 的编程步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;加载数据库的驱动： &lt;/p&gt;

    &lt;p&gt;Class.forName(driverClass),dirverClass对应数据库驱动类的字符串。如果针对mysql就是: Class.forName(“com.mysql.jdbc.Driver”);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过DriverManager来获取数据库的连接，方法如下：&lt;/p&gt;

    &lt;p&gt;DriverManager.getConnection(String url, String user, String pass)&lt;/p&gt;

    &lt;p&gt;url/user/pass 分别代表，数据库的连接地址，用户名和密码，url针对不同的数据库有不同的地址，这个可以针对每个数据库的jdbc文档进行查看，如果针对mysql的数据库，可以使用如下的方式来连接数据：&lt;br /&gt;
 jdbc:mysql://hostname:port/databasename&lt;br /&gt;
 如果是针对oracle的连接，则url地址如下：&lt;br /&gt;
 jdbc:oracle:thin:@hostname:port:databasename&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过Connection对象创建Statement对象。Connection来创建Statement方法有如下三个：&lt;/p&gt;

    &lt;p&gt;createStatement(): 创建Statement对象&lt;br /&gt;
 prepareStatement(String sql): 根据传入的sql语句，通过数据的预编译，创建预编译的Statement对象。&lt;br /&gt;
 prepareCall(Stirng sql): 根据传入的sql语句创建预编译的CallableSattement对象。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用Statement来执行sql语句，但是执行的时候有如下的三种方式：&lt;/p&gt;

    &lt;p&gt;execute()：可以执行任何的sql语句&lt;br /&gt;
 executeUpdate(): 主要执行DML DDL 返回受sql影响的行数，执行DCL的时候返回0&lt;br /&gt;
 executeQuery() 执行查询语句，返回结果集ResultSet对象&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果是查询语句，返回的是ResultSet对象，针对ResultSet对象主要提供如下的操作，next() previous() last() beforeFirst() afterLast() 等方法来移动指针。同时可以针对每一条记录可以使用getXxx()方式来获取数据。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;回收数据库的资源。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;p&gt;简单的例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package mysqltest;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;


public class MysqlMain {
	public static void main(String[] args) {
		try {
			Class.forName(&quot;com.mysql.jdbc.Driver&quot;);
		} catch (ClassNotFoundException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		Connection conn = null;
		Statement stat = null;
		ResultSet result = null;
		try {
			conn = DriverManager.getConnection(&quot;jdbc:mysql://127.0.0.1:3306/test&quot;,
					&quot;root&quot;, &quot;root&quot;);
			 stat = conn.createStatement();
			 result = stat.executeQuery(&quot;select * from student&quot;);
			 while(result.next()) {
				 System.out.println(result.getString(1) + &quot;  &quot; + result.getString(2));
			 }
			 result.close();
		 	 stat.close();
		 	 conn.close();
		} catch (SQLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;JDBC 对事务的支持：&lt;/p&gt;

&lt;p&gt;在使用事务的时候需要关闭掉Connection的自动提交，因为Connection的自动提交功能是自动打开的，所以需要关闭，关闭的方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conn.setAutoCommit(false)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在执行完sql语句后，需要手动的实现commit，语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conn.commit()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在遇到执行失败的时候可以使用conn.rellback() 来回滚事务。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;批量更行是针对有大量的更新操作，这个时候可以一次性全部执行，而不是一条条的执行，这会大大的加大性能。&lt;/p&gt;

&lt;p&gt;比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysqlStmt.setTimestamp(23, oracleResult.getTimestamp(&quot;LOG_TIME&quot;));
.......
.......
mysqlStmt.addBatch();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysqlStmt.executeBatch();
&lt;/code&gt;&lt;/pre&gt;

</description>
				<pubDate>Sat, 01 Aug 2015 00:00:00 +0800</pubDate>
				<link>/2015/08/01/Java%E7%AC%94%E8%AE%B07.html</link>
				<guid isPermaLink="true">/2015/08/01/Java%E7%AC%94%E8%AE%B07.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记6</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;异常&lt;/h4&gt;

&lt;p&gt;Java把所有的非正常信息分成两类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;异常&lt;/li&gt;
  &lt;li&gt;错误&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两类异常都是继承于Throwable。&lt;/p&gt;

&lt;p&gt;Java异常捕获有一个特点就是，把小异常放在catch块的前面进行先处理，然后再捕获大异常，把大异常放在catch块的后面。&lt;/p&gt;

&lt;p&gt;有的时候在try块里面打开的物理资源（例如：数据库连接、网络连接和磁盘文件等），这些物理资源都必须显示的回收。Java的垃圾回收机制是不会回收任何的物理资源，垃圾回收机制只能回收堆内存中的对象所占用的内存。这就必须使用finally来完成垃圾的回收。&lt;/p&gt;

&lt;p&gt;Java的异常也被分成两类：Checked异常和Runtime异常。所有的RuntimeException类及其子类的实例都被称为Runtime异常，不是RuntimeException类及其子类的异常实例则被称为Checked异常。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自定义异常&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果是自定义检查性异常则需要继承Exception基类，然后提供一个无参的构造器，以及一个带字符串的有参数构造器，并且将这个字符串参数作为该异常对象的描述信息。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class ExceptionTest extends Exception {
	public ExceptionTest() {}
	public ExceptionTest(String msg) {
		super(msg);
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果定义的是非检查性异常（也就是运行时异常），就需要自定义的类继承RuntimeException类，并且提供两个构造器，提供的形式如检查性异常一样。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class RuntimeExceptionTest extends RuntimeException {
	public RuntimeExceptionTest() {}
	public RuntimtExceptionTest(String msg) {super(msg);}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于异常信息的处理一般的做大是：层与层之间的异常信息有非常清晰的划分，上层功能的实现严格依赖于下层的API，但是不会跨界访问。例如：当业务逻辑层出现SQLException异常的时候，程序不应该把底层的SQLException异常传到用户界面，通常的做法是在程序中先捕获原始的异常信息，然后抛出一个新的业务异常，新的业务异常中包含了对用户的提示信息。这种处理方式被称为异常转译。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void exceptionMethod() throws SalException {

	try {
	...
	} catch(SQLException e) {
		// 做一些处理
		...
		// 包装异常，抛出一个新的异常
		throw new SalException(...);
	}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Sat, 11 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/11/Java%E7%AC%94%E8%AE%B06.html</link>
				<guid isPermaLink="true">/2015/07/11/Java%E7%AC%94%E8%AE%B06.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记5</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;泛型&lt;/h4&gt;

&lt;p&gt;所谓泛型就是在定义类、接口、方法时使用类型形参，这个类型形参将在声明变量、创建对象、调用方法时动态的指定。虽然在使用集合类，比如：List、Set等类时，会使用泛型，但是根据定义并不是只针对集合，在定义类的时候就可以指定泛型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Person&amp;lt;T&amp;gt; {
	private T info;
	public Person(T info) {
		this.info = info;
	}
}

Person&amp;lt;String&amp;gt; person = new Person&amp;lt;&amp;gt;(&quot;name&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当创建了带泛型声明的接口、父类的时候，可以为该类接口创建实现类、或从该父类派生出子类。但需要注意的是，当使用这些泛型接口、父类的时候不能再包含类型形参。例如下面是一种错误的描述：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这是一种错误的描述
public class Student extend Person&amp;lt;T&amp;gt; {

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用如下的方式来继承该类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Student extend Person&amp;lt;String&amp;gt; {

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者不指定泛型类型，但是这样会有警告信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Student extend Person {

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有的时候我们在程序中需要使用泛型的父类，这个时候可以使用如下方式实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这种用法有警告信息
public void test(List list) {
	....
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式是正确的，但是使用这种方式可能会引起警告信息，如果采用下面的方式来使用就会引起错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这种用法是错误的
public void test(List&amp;lt;Object&amp;gt; list) {

	...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Java中为了表示泛型类的父类，需要使用类型通配符，就是用？来表示。在上面的例子中可以使用如下的形式来表示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这种表示方式表示所有List类的父类，没有错误也没有警告信息
public void test(List&amp;lt;?&amp;gt; list) {

	...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是这里存在一个问题，如果使用List&amp;lt;?&amp;gt;这种父类，在程序中使用的话还需要强制类型的转换，转换成需要的类型，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void test(List&amp;lt;?&amp;gt; list) {	
	for (Object obj : list) {
		String str = (String)obj;
		...
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种使用方式还需要把对象强制类型转换一下，为了解决这种问题，可以给类型通配符设定一个上线：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void test(List&amp;lt;? extends xxx&amp;gt; list) {
	
	...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式就可以把List中的对象当成xxx类型，直接访问。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在定义泛型类的时候并不希望这个泛型类型是随意添加的，需要指定该类型是某个类型的子类，这个时候需要指定泛型类型的上限。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了解决这个问题需要指定类型形参的上限，例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Question&amp;lt;T extends Person&amp;gt; {

	...
	
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候在使用类型T的时候就必须传入Person的子类。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;泛型方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;泛型方法的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;修饰符 &amp;lt;T, S&amp;gt; 返回值类型 方法名(形参列表) {

...

}

T 和 S 都可以使用这种形式： T extends E，表示E是T的上限。

// 例子如下：
public &amp;lt;T&amp;gt; void fromArrayToCollection(T[] a, Collection&amp;lt;T&amp;gt; c) {
	for (T o : a) {
		c.add(o);
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果可以使用泛型方法，那泛型方法和类型通配符之间存在怎样的区别：在大多数的情况下都可以使用泛型方法来代替类型通配符。通配符就是被设计用来支持灵活的子类化的，但是泛型方法就是允许类型形参被表示方法的一个或者多个参数之间的类型依赖关系，或者方法返回值与参数之间的类型依赖关系。&lt;/p&gt;

</description>
				<pubDate>Fri, 10 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/10/Java%E7%AC%94%E8%AE%B05.html</link>
				<guid isPermaLink="true">/2015/07/10/Java%E7%AC%94%E8%AE%B05.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记4</title>
				<description>&lt;h4 id=&quot;section&quot;&gt;集合&lt;/h4&gt;

&lt;p&gt;Java的集合大致可以分成Set List 和 Map三种体系结构。其中Set代表无序不可重复的集合；List代表有序、重复的集合；而Map代表具有映射关系的集合。&lt;/p&gt;

&lt;p&gt;Java的集合类主要由两个接口派生而来：Collection 和 Map，这两个接口是根接口。Set 和 List两个接口派生于Collection接口。根据Set List 和 Map三种类型的特点，可以得出这样的结论：&lt;/p&gt;

&lt;p&gt;如果访问List集合中的元素，可以直接使用元素的索引；如果访问Map集合需要根据key来访问，访问Set集合则只能根据元素本身。使用比较多的是：
HashSet TreeSet ArrayList ArrayDeque LinkedList HashMap TreeMap&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Set&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Set集合不允许包含相同的元素，如果把两个相同的元素添加到Set集合中，则会添加失败。Set判断两个对象相同不是使用==运算符，而是根据equals方法，也就是说两个对象用equlas方法判断返回true，则可以添加成功。Set的子类包含：HashSet 、 TreeSet 、 EnumSet&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HashSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;HashSet具有以下几个特点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不能保证元素的排列顺序，遍历的元素可能与添加的元素顺序可能不一样。&lt;/li&gt;
  &lt;li&gt;HashSet不是同步的，如果两个以上的线程来修改HashSet则需要手动的保证代码的同步。&lt;/li&gt;
  &lt;li&gt;集合元素可以为null&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HashSet中存入一个元素的时候，系统会根据该元素的hashCode()返回的数字决定存储的位置。如果两个元素根据equals方法返回true，但是hashCode不一样，则HashSet同样会将两个对象添加成功。&lt;/p&gt;

&lt;p&gt;因此HashSet判断两个对象是否相等，需要equals返回true，同时hashCode返回的数值也一样。应该在重写equlas方法的时候，应该也需要重写hashCode方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LinkedHashSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LinkedHashSet 是HashSet的子类，它也是根据hashCode值来决定元素的存储位置，但是它不同的是需要使用链表来维护元素的次序。这样使得元素看起来是以插入的顺序保存的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TreeSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TreeSet是SortedSet接口的实现类，因此它可以确保TreeSet中的元素都是有序的。&lt;/p&gt;

&lt;p&gt;TreeSet集合采用hash算法来决定元素的存储位置，同时TreeSet采用红黑树的数据结构来存储集合元素。TreeSet采用了两种排序方法：自然排序和定制排序。&lt;/p&gt;

&lt;p&gt;自然排序是根据添加到TreeSet中的对象的compareTo(Object obj)来判断。因此在加入到TreeSet中的所有对象都要实现Comparable接口。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Test implements Comparable {
	...
	
	public int comparaTo(Object obj) {
	
		...
	
	}
	...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个Test对象就可以添加到TreeSet集合中。&lt;/p&gt;

&lt;p&gt;定制排序：&lt;/p&gt;

&lt;p&gt;定制排序是创建TreeSet对象的时候指定比较方法，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TreeSet ts = new TreeSet(new Comparator() {
	
	public int compare(Object o1, Object o2) {
		...
	}
	
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样在添加到TreeSet中的对象就不需要自己实现Comparable接口，因为TreeSet已经帮你实现了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ArrayList Vector&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ArrayList &amp;amp;&amp;amp; Vector 都是基于数组实现的List类。他们都是内部封装了一个动态可分配的数组，使用initialCapacity参数来设置该数组的长度，当向ArrayList和Vector中添加的元素超过了数组的长度则initialCapacity将会自动增长。&lt;/p&gt;

&lt;p&gt;ArrayList 与 Vector之间的主要区别是：ArrayList是线程不安全的，当多个线程修改同一个ArrayList集合的时候，需要手动的保证线程的安全。Vector是线程安全的。&lt;/p&gt;

&lt;p&gt;在Arrays中提供了asList(….)方法，把指定的数组转成List集合。&lt;strong&gt;但是需要注意的是转成后的List对象是不可以使用add remove等操作的，只能遍历。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LinkedList&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LinkedList 是List的一个集合，同时还实现了Deque接口，因此它可以被当成双端队列来使用。自然也可以被当成栈来使用。LinkedList内部使用的是链表的形式来保存集合中的元素。因此随机访问的性能不好，但是插入和删除的性能比较好。&lt;/p&gt;

&lt;p&gt;-Xms 是设置JVM的堆内存初始值大小。
-Xmx 是设置JVM的堆内存最大大小。&lt;/p&gt;

&lt;p&gt;总结：如果需要随机访问的性能，则考虑使用ArrayList，如果需要插入、删除的性能，则需要考虑使用LinkedList.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hashtable &amp;amp;&amp;amp; HashMap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;两者之间的区别：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hashtable 是一个线程安全的类，HashMap线程不安全。&lt;/li&gt;
  &lt;li&gt;Hashtable 不允许使用null作为key和value，但是HashMap是可以使用null作为key和value的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此 HashMap中只能有一个key位null，但是可以有无数的value值为null。&lt;/p&gt;

&lt;p&gt;与LinkedHashSet一样，Map中也存在一个LinkedHashMap，用链表来记录map对象的添加顺序。&lt;/p&gt;

&lt;p&gt;SortedMap 与 TreeMap 也是可以排序的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;自然排序： TreeMap 的所有key必须实现Comparable接口，而且所有的key应该是同一个类的对象，否则会抛出异常信息。&lt;/li&gt;
  &lt;li&gt;定制排序： 创建TreeMap对象的时候，传入一个Comparator对象，该对象负责对TreeMap中所有的key进行排序。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Collections&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Java 为 Set List Map 提供了一个工具类： Collections，它提供了大量方法对集合元素进行排序，查询和修改等操作。还提供了将集合元素对象设置为不可变、对集合对象实现同步的控制方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reverse / shuffle / sort / swap / rotate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Collections 提供了多个synchronizedXxx()方法，将集合包装成线程同步的集合。从而解决多线程并发的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Collection c = Collecionts.synchronizedCollection(new ArrayList());
List list = Collecionts.synchronizedList(new ArrayList());
Set set = Collecionts.synchronizedSet(new HashSet());
Map m = Collecionts.synchronizedMap(new HashMap());
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Thu, 09 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/09/Java%E7%AC%94%E8%AE%B04.html</link>
				<guid isPermaLink="true">/2015/07/09/Java%E7%AC%94%E8%AE%B04.html</guid>
			</item>
		
			<item>
				<title>spark-jobserver的安装</title>
				<description>&lt;h4 id=&quot;spark-jobserver&quot;&gt;spark-jobserver的安装&lt;/h4&gt;

&lt;p&gt;spark-jobserver 提供了一个RESTful接口来提交和管理spark的jobs,jars和job contexts。该工程位于：https://github.com/spark-jobserver/spark-jobserver&lt;/p&gt;

&lt;p&gt;特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;针对job 和 contexts的各个方面提供了REST风格的api接口进行管理&lt;/li&gt;
  &lt;li&gt;支持SparkSQL，Hive，Streaming Contexts/jobs 以及定制job contexts!&lt;/li&gt;
  &lt;li&gt;支持压秒级别低延迟的任务通过长期运行的job contexts&lt;/li&gt;
  &lt;li&gt;可以通过结束context来停止运行的作业(job)&lt;/li&gt;
  &lt;li&gt;分割jar上传步骤以提高job的启动&lt;/li&gt;
  &lt;li&gt;异步和同步的job API，其中同步API对低延时作业非常有效&lt;/li&gt;
  &lt;li&gt;支持Standalone Spark和Mesos&lt;/li&gt;
  &lt;li&gt;Job和jar信息通过一个可插拔的DAO接口来持久化&lt;/li&gt;
  &lt;li&gt;命名RDD以缓存，并可以通过该名称获取RDD。这样可以提高作业间RDD的共享和重用&lt;/li&gt;
  &lt;li&gt;支持scala 2.10 和 2.11&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在生产环境上一般直接部署spark-jobserver，部署的过程可以参考官方文档的Deployment这个章节。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;拷贝spark-jobserver 目录下的conf/local.sh.template文件为local.sh，cp conf/local.sh.template conf/local.sh.&lt;/li&gt;
  &lt;li&gt;配置conf/local.sh文件，主要是以下三个参数SPARK_VERISON / SPARK_HOME/ DRIVER_MEMORY&lt;/li&gt;
  &lt;li&gt;然后执行bin/server_deploy.sh config/local开始部署配置环境，这个时候可能会提醒你找不到local.sh文件，你可以根据错误提示信息把local.sh文件拷贝到相应的路径下。&lt;/li&gt;
  &lt;li&gt;在执行完server_deploy.sh文件后，可以执行server_start.sh文件进行启动jobserver。可能在启动的过程提示log4j找不到，这个时候需要把conf目录下的log4j文件拷贝到指定的位置。同时如果提示：spark-jobserver-master/bin/spark-job-server.jar does not exist，则个时候需要找一下当前路径下的spark-job-server.jar文件在哪个路径下，然后把该文件拷贝到bin目录下。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;启动没有报错的情况下，可以用ps -ef&lt;/td&gt;
          &lt;td&gt;grep job来查看一下当前是否存在jobserver的进程。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;正确启动后就可以通过浏览器访问该主机的8090端口，例如：192.168.1.100:8090。&lt;/li&gt;
  &lt;li&gt;可以在该主机上执行：curl –data-binary @job-server-tests/target/job-server-tests-$VER.jar localhost:8090/jars/test，把测试jar包放到jobserver上。&lt;/li&gt;
  &lt;li&gt;然后可以在Spark上执行任务：curl -d “input.string = a b c a b see” ‘localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample’&lt;/li&gt;
  &lt;li&gt;执行完后可以查看执行的结果：curl localhost:8090/jobs/5453779a-f004-45fc-a11d-a39dae0f9bf4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;jobserver的安装就完成。具体如何让编写的spark应用程序可以被jobserver调用，这需要创建一个jobserver的工程。&lt;/p&gt;
</description>
				<pubDate>Tue, 07 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/07/spark-jobserver%E7%9A%84%E5%AE%89%E8%A3%85.html</link>
				<guid isPermaLink="true">/2015/07/07/spark-jobserver%E7%9A%84%E5%AE%89%E8%A3%85.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记3</title>
				<description>&lt;h4 id=&quot;string--stringbuffer--stringbuilder---&quot;&gt;String / StringBuffer / StringBuilder / 时间 的使用&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;String StringBuffer StringBuilder&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;String 类是不可变类，一旦创建以后，包含在这个对象中的字符序列是不可以改变的，直至这个对象被销毁。&lt;/p&gt;

&lt;p&gt;StringBuffer 和 StringBuilder则是代表一个字符序列可变的类，他们提供了append() insert() reverse() setChartAt() setLength() 方法来改变这个字符序列。&lt;/p&gt;

&lt;p&gt;StringBuffer 和 StringBuilder两个类的用法基本相似，但是StringBuffer是线程安全的，而StringBuilder则没有实现线程安全的功能。所以StringBuilder的性能要高一些。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Date类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Java 提供Date类来处理日期与时间。由于Date存在的时间比较悠久导致很多构造方法都已经过时，不再推荐使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Date()
Date(long date)
boolean after(Date when)
.........before.........
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于Date的过时，现在大部分都是使用Calendar工具类。Calendar是一个抽象类，不能直接实例化。但是它提供了几个静态的getInstance()方法来获取Calendar对象。&lt;/p&gt;

&lt;p&gt;Calendar 与 Date之间的转换。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Calendar cl = Calendar.getInstance();
Date date = cl.getTime();

/**
 * 
 * Calendar 提供了add、set方法，field的类型有：Calendar.YEAR Calendar.MONTH....
 * 但是需要注意的是： Calendar.MONTH字段代表的月份，月份的起始值不是1，而是0。
 * @author wangmin
 *
 */
public class CalendarTest {
	public static void main(String[] args) {
		Calendar c = Calendar.getInstance();
		System.out.println(c.get(Calendar.YEAR));
		System.out.println(c.get(Calendar.MONDAY));
		System.out.println(c.get(Calendar.MONTH));
	}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Mon, 06 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/06/Java%E7%AC%94%E8%AE%B03.html</link>
				<guid isPermaLink="true">/2015/07/06/Java%E7%AC%94%E8%AE%B03.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记2</title>
				<description>&lt;h4 id=&quot;singleton-----&quot;&gt;Singleton / 不可变类 / 缓存不可变类 的实现&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Singleton（单例类）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;单例类用的地方很多，如果一个类始终只能创建一个实例，则这个类被称为单例类。&lt;/p&gt;

&lt;p&gt;以下程序是创建一个单例类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SingletonTest {
	public static void main(String[] args) {
		Singleton s1 = Singleton.getInstance();
		Singleton s2 = Singleton.getInstance();
		System.out.println(s1 == s2);
		s1.setName(&quot;Singleton1&quot;);
		System.out.println(&quot;s1 = &quot; + s1.getName());
		System.out.println(&quot;s2 = &quot; + s2.getName());
		s2.setName(&quot;Singleton2&quot;);
		System.out.println(&quot;s1 = &quot; + s1.getName());
		System.out.println(&quot;s2 = &quot; + s2.getName());
	}
}
class Singleton {
	private String name;
	private static Singleton singleton = new Singleton(&quot;default&quot;);
	private Singleton(String name) {
		this.name = name;
	}
	public static Singleton getInstance() {
		return singleton;
	}
	public void setName(String name) {
		this.name = name;
	}
	public String getName() {
		return this.name;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;不可变类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;不可变类就是在创建该类的实例后不能更改该类的属性。Java提供的8个包装类和java.lang.String都是不可变类。&lt;/p&gt;

&lt;p&gt;创建不可变类的几个规则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用private 和 final 修饰符修饰属性；&lt;/li&gt;
  &lt;li&gt;提供带参数的构建起，用于初始化属性值；&lt;/li&gt;
  &lt;li&gt;只提供getter方法；&lt;/li&gt;
  &lt;li&gt;重写hashCode 和 equlas方法；&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Person 类是一个不可变类
 * @author wangmin
 *
 */
public class Person {
	private final String name;
	private final int age;
	public Person() {
		this.name = &quot;&quot;;
		this.age = 0;
	}
	public Person(String name, int age) {
		this.name = name;
		this.age = age;
	}
	public String getName() {
		return this.name;
	}
	public int getAge() {
		return this.age;
	}
	public boolean equlas (Object obj) {
		if (this == obj) {
			return true;
		}
		if (obj != null &amp;amp;&amp;amp; obj.getClass() == this.getClass()) {
			Person p = (Person)obj;
			if (this.getName().equals(p.getName()) &amp;amp;&amp;amp; this.age == p.getAge()) {
				return true;
			}
		}
		return false;
	}
	public int hasCode() {
		return name.hashCode() + age;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;缓存不可变类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果程序经常需要使用相同的不可变类实例，则应该考虑缓存这种不可变类。下面是简单的使用数组的方式实现一个不可变类。&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 使用数组的方式来缓解Person
 * 1. 定义数组大小，并且使用private static 修饰
 * 2. 定义valueOf方法获取对象，但是其中存在以下几种情况：
 * 	2.1 循环遍历整个缓存数组的大小，然后比较，如果已经被缓存则直接取出。（注意数组元素可能为null）
 *  2.2 由2.1的结果可知，如果数组中没有则新创建。
 *  2.3 首次判断pos是否已满，如果已满则将新的对象创建到数组0的位置，并且将pos设置为0，并返回。
 *  2.4 如果没有满则根据pos的位置创建，并返回。
 * @author wangmin
 *
 */
public class CachePerson {
	private static int MAX_SIZE = 10;
	private static CachePerson[] cp = new CachePerson[MAX_SIZE];
	private String name;
	private static int pos = 0;
	
	public CachePerson() {
		
	}
	public CachePerson(String name) {
		this.name = name;
	}
	public String getName() {
		return this.name;
	}
	public static CachePerson valueOf(String name) {
		for (int i = 0; i &amp;lt; MAX_SIZE; i++) {
			if (cp[i] != null &amp;amp;&amp;amp; cp[i].getName().equals(name)) {
				return cp[i];
			}
		}
		if (pos == MAX_SIZE) {
			cp[0] = new CachePerson(name);
			pos = 1;
		} else {
			cp[pos++] = new CachePerson(name);
		}
		return cp[pos-1];
	}
	public boolean equlas (Object obj) {
		if (this == obj) {
			return true;
		}
		if (obj != null &amp;amp;&amp;amp; obj.getClass() == this.getClass()) {
			CachePerson cp = (CachePerson)obj;
			if (cp.getName().equals(this.getName())) {
				return true;
			}
		}
		return false;
	}
	public int hasCode() {
		return name.hashCode();
	}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Sun, 05 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/05/Java%E7%AC%94%E8%AE%B02.html</link>
				<guid isPermaLink="true">/2015/07/05/Java%E7%AC%94%E8%AE%B02.html</guid>
			</item>
		
			<item>
				<title>Java学习笔记1</title>
				<description>&lt;h4 id=&quot;tostring----equals-&quot;&gt;toString / == / equals 的使用&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;toString的使用&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Person p = new Person();
System.out.println(p);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行上面的程序得到的结果是：Person@f72645&lt;/p&gt;

&lt;p&gt;其实输出这个结果就是调用了对象的toString()方法，也就是其结果和调用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;System.out.println(p.toString())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;toString是一个自我描述的方法，一般都是在直接打印该对象的时候，输出该对象的描述信息。要实现这个自我描述方法，需要自己重写toString方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Person {
	private String name;
	private double weight;
	....
	publci String toString() {
		return &quot;name: &quot; + name + &quot;, weight: &quot; + weight;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的Person类就是重写了toString方法，在打印该对象的时候，就是打印：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: xxxx, weight: xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;equlas方法和==的区别&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;判断两个对象是否相等可以使用equals方法，另一种是使用==判断。&lt;/p&gt;

&lt;p&gt;使用 == 判断两个对象是否相等的条件是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果两个对象都是基本类型，那么他们的值相等就返回true&lt;/li&gt;
  &lt;li&gt;如果非基本类型，则说明他们的引用需要指向同一个对象，这个时候才返回true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int i = 100;
float f = 100.0f;
i == f 返回true

String str1 = new String(&quot;hello&quot;);
String str2 = new String(&quot;hello&quot;);
str1 == str2 这个将输出false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而equlas是Object提供的一个方法，使用这个方法来判断两个对象是否相等，和使用 == 来判断是一样的效果，但是equlas方法是可以被子自定义的类重写的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Person {

	...
	
	public boolean equlas(Object obj) {
		... 
		return true;
	}
} 在使用equlas比较的时候就会调用自定义的equlas方法，提供一个标准的equlas方法重写的模板：

public boolean equlas(Object obj) {

	if (this == obj) {
		return true;
	}
	if (obj != null  &amp;amp;&amp;amp; obj.getClass() == Person.class) {
		Person pobj = (Person)obj;
		if (...) {
			return true;
		}
	}
	return false;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Object 默认提供的equlas只是比较对象的地址，即Object类的equlas方法比较的结果与 == 运算符比较的结果是完全相同。因此，在实际运用中需要重写equals，这时候可以根据自己的条件来决定如何判断相等。&lt;/p&gt;

</description>
				<pubDate>Thu, 02 Jul 2015 00:00:00 +0800</pubDate>
				<link>/2015/07/02/Java%E7%AC%94%E8%AE%B01.html</link>
				<guid isPermaLink="true">/2015/07/02/Java%E7%AC%94%E8%AE%B01.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记4</title>
				<description>&lt;h4 id=&quot;standalone&quot;&gt;任务的提交以及Standalone集群模式的部署&lt;/h4&gt;

&lt;h4 id=&quot;spark-submit&quot;&gt;spark-submit&lt;/h4&gt;

&lt;p&gt;首先需要打包代码，如果你的代码需要依赖其他的包环境则需要单独的打包这些依赖，应为cluster会将所有依赖的jar包分发到各个节点上进行使用。推荐的方法是将依赖包和程序都统一的打成一个包，这样就可以直接使用spark-submit方法来运行，具体的pom.xml配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependencies&amp;gt;
	&amp;lt;dependency&amp;gt; &amp;lt;!-- Spark dependency --&amp;gt;
		&amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;5.1.13&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&amp;lt;build&amp;gt;
	&amp;lt;plugins&amp;gt;
		&amp;lt;plugin&amp;gt;
			&amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;maven-compiler-plugin&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;2.3.2&amp;lt;/version&amp;gt;
			&amp;lt;configuration&amp;gt;
				&amp;lt;!-- 使用1.7 jdk进行编译 --&amp;gt;
				&amp;lt;source&amp;gt;1.7&amp;lt;/source&amp;gt;
				&amp;lt;target&amp;gt;1.7&amp;lt;/target&amp;gt;
			&amp;lt;/configuration&amp;gt;
		&amp;lt;/plugin&amp;gt;
		&amp;lt;plugin&amp;gt;
			&amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;2.5.5&amp;lt;/version&amp;gt;
			&amp;lt;configuration&amp;gt;
				&amp;lt;descriptorRefs&amp;gt;
					&amp;lt;descriptorRef&amp;gt;jar-with-dependencies&amp;lt;/descriptorRef&amp;gt;
				&amp;lt;/descriptorRefs&amp;gt;
			&amp;lt;/configuration&amp;gt;
			&amp;lt;executions&amp;gt;
				&amp;lt;execution&amp;gt;
					&amp;lt;id&amp;gt;make-assembly&amp;lt;/id&amp;gt;
					&amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
					&amp;lt;goals&amp;gt;
						&amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;
					&amp;lt;/goals&amp;gt;
				&amp;lt;/execution&amp;gt;
			&amp;lt;/executions&amp;gt;
		&amp;lt;/plugin&amp;gt;
	&amp;lt;/plugins&amp;gt;
&amp;lt;/build&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spark &amp;amp;&amp;amp; hadoop 的scope值都设置为provided，然后打包的过程是把需要依赖的第三方东西打成一个统一的jar包去运行，使用的是maven-compiler-plugin插件。&lt;/p&gt;

&lt;p&gt;在服务器上提交的命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./bin/spark-submit \
  --class &amp;lt;main-class&amp;gt;
  --master &amp;lt;master-url&amp;gt; \
  --deploy-mode &amp;lt;deploy-mode&amp;gt; \
  --conf &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; \
  ... # other options
  &amp;lt;application-jar&amp;gt; \
  [application-arguments]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spark-submit 可以加载一个配置文件，默认是加载在conf/spark-defaults.conf&lt;/p&gt;

&lt;h4 id=&quot;spark-standalone-mode&quot;&gt;Spark Standalone Mode&lt;/h4&gt;

&lt;p&gt;除了运行在Mesos和YARN集群之外，spark也提供了简单的独立部署模式。可以通过手动的启动master和worker，也可以通过spark提供的启动脚本来启动。独立部署也可以通过运行在一个机器上，进行测试。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;为了安装你需要放置一个编译好的spark版本到每个机器上。&lt;/li&gt;
  &lt;li&gt;启动集群有两种方式，一种是手动启动，另一种是通过启动脚本启动。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;手动启动spark集群&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;启动一个独立的master可以使用如下的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./sbin/start-master.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一旦启动可以通过访问：http://localhost:8080端口访问master，打出的spark://HOST:PORT 的URL，work节点可以使用这个链接连到master上。&lt;/p&gt;

&lt;p&gt;可以使用如下的命令来使worker节点连接到master上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./sbin/start-slave.sh &amp;lt;worker#&amp;gt; &amp;lt;master-spark-URL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;worker在加入到master后可以访问master的http://localhost:8080，可以清洗的看到被加入的worker节点的信息。&lt;/p&gt;

&lt;p&gt;在启动master和worker的时候可以带上参数进行设置，参数的列表如下:其中比较重要的是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;-c CORES， 这个是指定多少个cpu分配给spark使用，默认是全部cpu&lt;/li&gt;
  &lt;li&gt;-m MEM，这个是指定多少的内存分配给spark使用，默认是全部的内存的减去1g的操作系统内存全部分配给spark使用。一般的格式是1000M or 2G&lt;/li&gt;
  &lt;li&gt;-d DIR, 这个指定spark任务的日志输出目录。&lt;/li&gt;
  &lt;li&gt;–properties-file FILE 指定spark指定加载的配置文件的路径默认是： conf/spark-defaults.conf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;脚本方式部署&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过spark的部署脚本部署首先需要在spark的主目录下创建一个conf/slaves的文件，这个文件中每一行代表一个worker的hostname.需要注意的是，master访问worker节点是通过SSH访问的，所以需要master通过ssh无密码的登录到worker，否则需要设置一个 SPARK_SSH_FOREGROUND的环境变量，这个变量的值就是每个worker的密码&lt;/p&gt;

&lt;p&gt;然后可以通过spark安装目录下的sbin/….sh文件进行启动，
如果要启动和停止master和slave可以使用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sbin/start-all.sh&lt;/li&gt;
  &lt;li&gt;sbin/stop-all.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;注意的是这些脚本必须是在master机器上执行&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;同时你可以通过配置集群的 conf/spark-env.sh文件来进一步配置集群的环境。但是这也文件需要通过拷贝conf/spark-env.sh.template文件来创建，并且需要把这个文件拷贝到所有的worker节点上。&lt;/p&gt;

&lt;p&gt;其中： SPARK_MASTER_OPTS &amp;amp;&amp;amp; SPARK_WORKER_OPTS 两个配置项比较复杂。&lt;/p&gt;

&lt;p&gt;通过在SparkContext构造器中传入spark://IP:PORT这个来启用这个集群。同时可以在交互式的方式启动脚本中使用：./bin/spark-shell –master spark://IP:PORT 来启动集群执行。&lt;/p&gt;

&lt;p&gt;独立部署模式的集群现在只是简单的支持FIFO调度。
为了允许多个并发用户，可以通过SparkConf设置每个应用程序需要的资源的最大数。默认情况下，它会请求使用集群的全部的核，而这只是同时运行一个应用程序才回有意义。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val conf = new SparkConf()
             .setMaster(...)
             .setAppName(...)
             .set(&quot;spark.cores.max&quot;, &quot;10&quot;)
val sc = new SparkContext(conf)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了可以在程序中指定你也可以在spark-env.sh中设置默认的值，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=&amp;lt;value&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;spark的高可用设置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;spark的高可用设置有两种，一种是通过Zookeeper来实现，另一种是通过本地文件系统来实现。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;使用ZooKeeper备份master，利用zookeeper提供的领导选举和状态保存，你可以让更多的master连接到zookeepre实例。一个将会被选举为leader其他的则会保存备份他的状态。如果master死掉，zookeeper可以选举一个新的leader，整个过程需要1到2分钟的时间，但是这个过程只会对新的任务调度有影响。为了使用这种方式需要的配置项为：SPARK_DAEMON_JAVA_OPTS，这个配置项有三个配置信息：spark.deploy.recoveryMode/spark.deploy.zookeeper.url/spark.deploy.zookeeper.dir&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用本地文件系统来恢复该节点。为了使用这种方式需要的配置项为：SPARK_DAEMON_JAVA_OPTS，这个配置项有两个配置信息：spark.deploy.recoveryMode、spark.deploy.recoveryDirectory&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Mon, 29 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/29/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04.html</link>
				<guid isPermaLink="true">/2015/06/29/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04.html</guid>
			</item>
		
	</channel>
</rss>
