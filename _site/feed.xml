<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Stylish Jekyll Theme</description>
		<link>/</link>
		<atom:link href="/" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Spark学习笔记2</title>
				<description>&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;每一个spark程序都是有一个驱动程序组成，并且通过main函数运行。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;spark有两个重要的抽象：&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;RDD，分布式弹性数据集，他是一个跨越多个节点的分布式集合。&lt;/li&gt;
      &lt;li&gt;另一个抽象是共享变量。spark支持两种类型的共享变量：一个是广播（broadcast variables）他可以缓存一个值在集群的各个节点。另一个是累加器（accumulators）他只能执行累加的操作，比如可以做计数器和求和。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Initializing Spark&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;运行spark程序，需要在程序中创建一个JavaSparkContext对象，但是在创建JavaSparkContext对象的时候需要指定SparkConf对象，这个对象是配置一些重要的参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;SparkConf conf = new SparkConf.setAppName(appName).setMaster(url);
JavaSparkContext sc = new JavaSparkContext(conf); ---
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;appName就是这个任务的名字，url代表spark集群的Url地址。这个地址不要写死，因为这样可以通过命令行来传入需要执行的spark集群，方便调试与线上集群运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Resilient Distributed Datasets (RDDs)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Spark的核心就是围绕着RDD，它是一个自动容错的分布式数据集合。他有两种方式创建，第一种就是在驱动程序中对一个集合进行并行化。第二种是来源于一个外部的存储系统。比如：共享系统、HDFS、HBase或者任何提供任何Hadoop 输入格式的数据源。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;第一种：Parallelized Collections 创建这个集合需要调用那个JavaSparkContext的parallelize方法来初始化一个已经存在的集合。&lt;/li&gt;
    &lt;/ul&gt;

    &lt;hr /&gt;

    &lt;pre&gt;&lt;code&gt;  List&amp;lt;Integer&amp;gt; data = Arrays.asList(1,2,3,4,5);
  JavaRDD&amp;lt;Iteger&amp;gt; distData = sc.parallelize(data);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;这就创建了一个并行的集合，在这个集合上可以执行	distData.reduce((a, b) -&amp;gt; a + b)&lt;/p&gt;

    &lt;p&gt;在并行数组中一个很重要的参数是partitions，它来描述数组被切割的数据集数量。Spark会在每一个partitions上运行任务，这个partitions会被spark自动设置，一般都是集群中每个CPU上运行2-4partitions，但是也可以自己设置，可以通过parallelize （e.g. sc.parallelize(data, 10)），在有些地方把partitions成为 slices。&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;External Datasets&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;JavaRDD&lt;string&gt; distFile = sc.textFile(&quot;data.txt&quot;); &lt;/string&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;textFile也可以设置partitions参数，一般都是一个block一个partitions，但是也可以自己设置，自己设置必须要不能少于block的数量。&lt;/p&gt;

    &lt;p&gt;针对Hadoop的其他输入格式，你能用这个JavaSparkContext.hadoopRDD方法，你需要设置JobConf和输入格式的类。也可以使用JavaSparkContext.newAPIHadoopRDD针对输入格式是基于“new”的MapReduceAPI&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RDD Operations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RDD的操作可以分成两类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. transformations 
2. actions
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;map就是一个transformation操作，他会针对数据集的每一个元素执行一个函数。然后返回一个新的RDD来替换掉原先的RDD。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;reduce就是一个action操作，他能够聚合所有的元素通过执行一个函数，然后返回给驱动程序一个最终的值。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;所有的transformations都是lazy操作，也就是说并不会马上计算结果，要等到执行一个action操作并且返回一个值给驱动程序的时候才会执行transformations操作。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;可以调用RDD的 persist (or cache) 方法来持久化RDD，这样的话在下次使用RDD的时候就会非常快。同时也支持持久化到磁盘，或者复制到多个节点。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;直接使用 rdd.foreach(println) 在local模式下是可行的，但是在cluster模式下是不行的，必须要执行collect()方法，将所有的数据拉取到本地，然后执行foreach()操作。如果是数据量比较小的话可以使用take方法，rdd.take(100).foreach(println)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 22 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/22/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.html</link>
				<guid isPermaLink="true">/2015/06/22/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.html</guid>
			</item>
		
			<item>
				<title>Junit使用笔记</title>
				<description>&lt;p&gt;Junit的所有测试方法都是以@Test修饰，以public void 开头。如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Test
public void testAdd() {
	assertEquals(0, new Calculate().add(0, 1));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@BeforeClass &amp;amp;&amp;amp; @AfterClass 都是只会执行一次，@BeforeClass是在类加载的时候执行，@AfterClass 是整个类结束的时候被执行，整个方法是一个静态方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@BeforeClass
public static void setUpBeforeClass() throws Exception {
	System.out.println(&quot;before class&quot;);
}

@AfterClass
public static void tearDownAfterClass() throws Exception {
	System.out.println(&quot;after class&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Before &amp;amp;&amp;amp; @After两个方法是在每个测试方法执行的执行都会被执行，@Before是在方法执行前执行，@After是在方法执行结束后执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Before
public void setUp() throws Exception {
	System.out.println(&quot;before&quot;);
}

@After
public void tearDown() throws Exception {
	System.out.println(&quot;after&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Ignore 所修饰的测试方法会被测试运行器忽略，例如以下的test1方法就会被测试运行器忽略执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Ignore
@Test
public void test1() {
	System.out.println(&quot;test1&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Test(timeout=毫秒），用来指定时间上限，如果这个测试方法的执行时间超过了这个时间值则测试失败。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 会执行失败，因为sleep的时间长于设定的timeout时间
@Test(timeout=1000)
public void test() {
	try {
		Thread.sleep(2000);
	} catch (InterruptedException e) {
		e.printStackTrace();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Test(expected=异常类)，用expected来指定应该抛出的异常，如果在执行过程中没有抛出异常或者抛出的异常不是指定的异常，则测试失败。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这个测试案例会执行成功，因为指定的异常就是程序要抛出的异常
@Test(expected=IndexOutOfBoundsException.class) 
public void outOfBounds() 
{
    new ArrayList&amp;lt;Object&amp;gt;().get(1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试套件就是组织所要测试的类一起运行，如果单个类单独的运行是比较麻烦的，可以使用测试套件一起运行这些测试类。需要注意的是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;测试套件的类是不包含其他任何方法&lt;/li&gt;
  &lt;li&gt;同时要更改测试运行器为Suite.class&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将要测试的类作为数组传入到SuiteClasses({})中&lt;/p&gt;

    &lt;hr /&gt;

    &lt;pre&gt;&lt;code&gt;  // 更改测试运行器以及将要测试的类放入SuiteClasses中
  @RunWith(Suite.class)
  @SuiteClasses({ AppTest.class, CalculateTest.class, JunitFlowTest.class })
  public class AllTests {
      // 没有测试方法
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 22 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/22/Junit%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</link>
				<guid isPermaLink="true">/2015/06/22/Junit%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</guid>
			</item>
		
			<item>
				<title>Spark学习笔记1</title>
				<description>&lt;p&gt;&lt;strong&gt;Apache Spark is a fast and general-purpose cluster computing system.&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;spark 提供了 Java  Scala Python and 的API。
在examples/src/main目录下有Java和Scala例子， 用 bin/run-example 运行。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过运行： ./bin/spark-shell –master local[2] 来进行交互式的操作，这是学习sprak最好的方式&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;从1.4起spark也提供了R的api，./bin/sparkR –master local[2]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;spark 提供了Scala 和 Python的交互式分析shell来学习api
./bin/spark-shell&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RDD是分布式弹性数据集，可以理解为就是一个分布式的集合，这个集合的创建可以通过Hadoop 的 InputFormats，或者通过其他RDD转换而来。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RDD有动作，他能够返回值，以及转换操作，他会返回一个指针指向新的RDD，通过RDD的filter操作返回一个新的RDD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RDD有很复杂的操作，他可以直接调用Scala Java的库方法，使用的时候需要使用：import java.lang.Math来引入。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Caching&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;spark 支持推送一个数据集到一个集群的缓存中，尤其是那种需要经常重复读取的数据。eg: linesWithSpark.cache()&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Self-contained Applications（独立的应用程序）&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建Maven项目。&lt;/li&gt;
  &lt;li&gt;pom.xml 文件如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependencies&amp;gt;
   &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
      &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
    &amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;创建一个简单的spark程序：&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;public class SimpleApp {
	public static void main(String[] args) {
		// 文件路径
		String logFile = &quot;/home/wm/apps/spark-1.4.0-bin-hadoop2.6/README.md&quot;;
		SparkConf conf = new SparkConf().setAppName(&quot;Simple Application&quot;);
		JavaSparkContext sc = new JavaSparkContext(conf);
		JavaRDD&amp;lt;String&amp;gt; logData = sc.textFile(logFile).cache();
		@SuppressWarnings(&quot;serial&quot;)
		long numAs = logData.filter(new Function&amp;lt;String, Boolean&amp;gt;() {
			public Boolean call(String s) throws Exception {
				return s.contains(&quot;a&quot;);
			}
			
		}).count();
		@SuppressWarnings(&quot;serial&quot;)
		long numBs = logData.filter(new Function&amp;lt;String, Boolean&amp;gt;() {

			public Boolean call(String s) throws Exception {
				return s.contains(&quot;b&quot;);
			}
			
		}).count();
		System.out.println(&quot;Lines with a: &quot; + numAs + &quot;, lines with b: &quot; + numBs);
		sc.close();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;mvn pckage&lt;/li&gt;
  &lt;li&gt;将target目录下的sparkdemo2-0.0.1-SNAPSHOT.jar文件放在spark的安装目录下。&lt;/li&gt;
  &lt;li&gt;在spark安装目录下执行jar包中的程序，但是要指定执行的class文件，这个class文件需要全路径。例如：&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;./bin/spark-submit --class &quot;com.wm.test.sparkdemo&quot; --master local[4] sparkdemo-0.0.1-SNAPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;

</description>
				<pubDate>Sun, 21 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/21/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.html</link>
				<guid isPermaLink="true">/2015/06/21/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.html</guid>
			</item>
		
			<item>
				<title>Hadoop的单机和集群部署</title>
				<description>&lt;h4 id=&quot;hadoop&quot;&gt;hadoop单机部署步骤：&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Required software&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Java(Java 7 以上）&lt;/li&gt;
  &lt;li&gt;SSH&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;download the latest version&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Local(Standalone) Mode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;hadoop可以直接独自运行在一个JVM程序里面。无需运行任何守护进程，所有程序都在同一个JVM上。在独立模式下测试和调试MapReduce程序很方便，因此该模式在开发阶段比较合适。在独立模式（本地模式）下将使用本地文件系统和本地MapReduce作业运行器。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pseudo-Distributed Mode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;可以使用伪分布式的方式运行hadoop，也就是所有的守护进程都是运行在一台机器上。&lt;/p&gt;

&lt;p&gt;配置文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etc/hadoop/core-site.xml:

&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;

etc/hadoop/hdfs-site.xml:

&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在需要使ssh localhost连接本地无需密码：&lt;/p&gt;

&lt;p&gt;ssh localhost&lt;/p&gt;

&lt;p&gt;如果执行这条命令需要密码，则需要执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ssh-keygen -t dsa -P &#39;&#39; -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下是一个例子，没有运行在yarn上。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先需要格式化文件系统：  bin/hdfs namenode -format&lt;/li&gt;
  &lt;li&gt;在后台启动一个namenode 和 datanode: sbin/start-dfs.sh，启动后可以在logs目录下查看日志，也可以访问： http://localhost:50070/&lt;/li&gt;
  &lt;li&gt;在hdfs中创建一个目录：bin/hdfs dfs -mkdir /user&lt;/li&gt;
  &lt;li&gt;拷贝本地文件到hdfs中：bin/hdfs dfs -put etc/hadoop/*.xml /user&lt;/li&gt;
  &lt;li&gt;执行程序：bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar grep /input /output ‘dfs[a-z.]+’&lt;/li&gt;
  &lt;li&gt;查看输出，可以先把hdfs中的文件拷贝到本地后查看：bin/hdfs dfs -get output output&lt;/li&gt;
  &lt;li&gt;cat output/*&lt;/li&gt;
  &lt;li&gt;或者直接在hdfs中查看： bin/hdfs dfs -cat output/*&lt;/li&gt;
  &lt;li&gt;停止hdfs可以使用： sbin/stop-dfs.sh&lt;/li&gt;
  &lt;li&gt;可以使用jps命令查看进程。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;使用： bin/hdfs dfs -rmdir /user/trssmas/output 删除指定的目录&lt;/li&gt;
  &lt;li&gt;使用： bin/hdfs dfs -rm /user/trssmas/output/file1 删除指定路径下的文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;在单个节点上运行YARN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先需要配置：etc/hadoop/mapred-site.xml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置： etc/hadoop/yarn-site.xml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候可以启动yarn了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sbin/start-yarn.sh
sbin/stop-yarn.sh
sbin/stop-all.sh 结束所有的进程
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;hadoop-1&quot;&gt;hadoop的集群部署步骤&lt;/h4&gt;

&lt;p&gt;在集群的部署和伪分布式的部署不太一样，具体操作如下：&lt;/p&gt;

&lt;p&gt;首先本地环境是两台物理机，ip地址分别为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;10.249.150.51 （master)&lt;/li&gt;
  &lt;li&gt;10.249.150.52  (slave)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;选取51作为master， 52作为slave&lt;/p&gt;

&lt;p&gt;hadoop的版本这次选取的是： hadoop-2.5.1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;准备工作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在安装之前需要分配一个hadoop用户专门用于hadoop的操作，同时在每台机器上安装JAVA(推荐使用oracle的JDK）和SSH。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;修改每台主机的用户名，51这台机器为master，所以修改这台机器的用户名为master。linux下修改主机的名字会随着使用的linux发行版不同而不同，但是如果只是暂时修改主机名（就是重启后用命令修改的主机名失效）可以在所有linux发行版中使用：hostname 修改的名字。但是推荐是永久修改主机名，这里举出Ubuntu和Centos两种发行版修改主机名的方式：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Ubuntu：直接编辑etc下的hostname文件： vi /etc/hostname 修改主机名。编辑完后需要重启才能生效。&lt;/li&gt;
      &lt;li&gt;Centos：直接编辑etc/sysconfig/network：将HOSTNAME修改为主机名，如果没有就添加一行： HOSTNAME=master。编辑完后需要重新启动才能生效。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改hosts文件，修改hosts文件后可以直接使用主机名代替ip地址访问其他机器。hosts文件的修改是通过编辑etc/hosts文件得到的，vi /etc/hosts文件。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  127.0.0.1 localhost
  10.249.150.51 master
  10.249.150.52 slave
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;通过修改hostname 和 hosts文件，这样我们可以通过访问主机名就能访问机器了，比如可以使用：&lt;/p&gt;

    &lt;p&gt;ping master 或者 ping slave 都能够ping通。&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：修改hostname和修改hosts文件每台机器上都要做，修改hostname是为了将每天机器的主机名改成该机器相应的名字，比如将52改成slave。修改hosts文件，每天机器都需要做相同的操作，比如：在51上要做： 10.249.150.51 master &amp;amp;&amp;amp; 10.249.150.52 slave，那在52上也要做相同的操作，这样才能在每台机器上ping通所有机器。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;配置master使用SSH无密码登录所有slave。这个操作要让master能够使用ssh无密码的访问所有slave。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;首先在master主机上，使用 ssh-keygen -t rsa 命令生成密钥对。&lt;/li&gt;
      &lt;li&gt;然后使用 cat ~/.ssh/id_rsa.pub » ~/.ssh/authorized_keys命令，配置ssh访问本地无需要密码。&lt;/li&gt;
      &lt;li&gt;完成后可以使用： ssh master检查一下是否需要密码才能登录，如果无需密码说明ssh设置正确，否则需要重新设置一次。&lt;/li&gt;
      &lt;li&gt;在完成了master的设置之后，需要把master上产生的公钥发送到每个slave机器上，命令如下： scp ~/.ssh/id_rsa.pub 用户名@slave的IP地址:~/.ssh/ 这条命令的意思是：使用scp命令将将id_rsa.pub拷贝到slave地址的~/.ssh目录下。&lt;/li&gt;
      &lt;li&gt;在slave机器下执行：cat ~/.id_rsa.pub » ~/.ssh/authorized_keys&lt;/li&gt;
      &lt;li&gt;在所有的slave节点操作完成后，可以在master节点上使用ssh slave来验证是否需要密码才能登录。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;设置hadoop的配置文件&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;需要在etc/hadoop文件中修改slaves文件，将文件中的内容全部换成所有slave的名字，注意，如果原文件中又localhost，就将localhost删掉，同时每行写一个slave机器的名字，如下所示：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; slave1
 slave2
 ....
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改core-site.xml文件，在core-site.xml文件中配置成如下：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;hdfs://master:9000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改hdfs-site.xml，如下所示：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;property&amp;gt;
  		&amp;lt;name&amp;gt;dfs.namenode.secondary.http-address&amp;lt;/name&amp;gt;
  		&amp;lt;value&amp;gt;Master:50090&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
  		&amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
  		&amp;lt;value&amp;gt;3&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改 mapred-site.xml文件，这个文件需要从模板中复制一份。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;property&amp;gt;
  		&amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
  		&amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改yarn-site.xml&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;property&amp;gt;
  		&amp;lt;name&amp;gt;yarn.resourcemanager.hostname&amp;lt;/name&amp;gt;
  		&amp;lt;value&amp;gt;Master&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
  		&amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
  		&amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;到此，hadoop的基本配置完成。接下来需要将这个配置的hadoop文件拷贝到所有slave机器上。&lt;strong&gt;注意：slave机器上的hadoop路径要和master上的hadoop路径一直。&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上诉步骤完成后就可以在master节点上启动hadoop了，使用的命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	bin/hdfs namenode -format // 这个是在hadoop第一次使用的时候执行，以后就不需要执行。
	sbin/start-dfs.sh // 启动hdfs
	sbin/start-yarn.sh // 启动yarn
	
	启动hdfs和启动yarn可以合并到一个命令：
	sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动完成后可以在master节点上使用： jps命令查看hadoop启动的进程。在slave节点上使用 jps可以查看slave节点上启动的hadoop进程。&lt;/p&gt;

&lt;p&gt;成功启动后（一般在启动的终端上没有出现warning信息一般为启动成功），可以使用http://master:50070/访问 DataNode和NameNode&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在集群上执行MapReduce例子，执行的例子可以参考伪分布式中的例子。在执行的过程中可以访问： http://master:8088/cluster来查看任务的执行情况。&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 16 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/16/Hadoop%E7%9A%84%E5%8D%95%E6%9C%BA%E5%92%8C%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html</link>
				<guid isPermaLink="true">/2015/06/16/Hadoop%E7%9A%84%E5%8D%95%E6%9C%BA%E5%92%8C%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html</guid>
			</item>
		
			<item>
				<title>Java中遍历Map的各种方式</title>
				<description>&lt;p&gt;在遍历Map集合之前首先先定义一个Map对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Map&amp;lt;String, String&amp;gt; map = new LinkedHashMap&amp;lt;String, String&amp;gt;();
map.put(&quot;1&quot;, &quot;one&quot;);
map.put(&quot;2&quot;, &quot;two&quot;);
map.put(&quot;3&quot;, &quot;three&quot;);
map.put(&quot;4&quot;, &quot;fore&quot;);
map.put(&quot;5&quot;, &quot;five&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个地方使用的是LinkedHashMap，主要是为了确保让map中的元素是按照插入的顺序存放的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 使用keySet()方法遍历&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用keyset方法遍历，是先取出map的key组成的Set集合，通过对Set集合的遍历，然后使用map.get(key)方法取出value值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (String key : map.keySet()) {
	System.out.println(key + &quot; : &quot; + map.get(key));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2. 使用map的values()方法遍历集合的values&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;map.values()返回的是由map的值组成的Collection，这个方法只能遍历map的所有value，不能得到map的key。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (String value : map.values()) {
	System.out.println(value);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3. 使用map的entrySet()方法遍历&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用map的entrySet()方法返回一个以Entry为元素的Set集合，然后对Set集合进行遍历。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (Entry&amp;lt;String, String&amp;gt; entry : map.entrySet()) {
	System.out.println(entry.getKey() + &quot; : &quot; + 	entry.getValue());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;4. 通过keySet()返回的集合的iterator遍历&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;由于map.keySet()返回的是一个Set集合，所以通过它的iterator()方法返回一个迭代器，通过迭代器遍历map。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iterator&amp;lt;String&amp;gt; it = map.keySet().iterator();
while(it1.hasNext()) {
	String key = it1.next();
	System.out.println(key + &quot; : &quot; + map.get(key));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;5. 通过values()返回的Collection的iterator遍历&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;map.values()方法返回的是一个Collection&lt;string&gt;对象，这个集合对象可以使用iterator方法访问。&lt;/string&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iterator&amp;lt;String&amp;gt; it = map.values().iterator();
while(it1.hasNext()) {
	String key = it1.next();
	System.out.println(key + &quot; : &quot; + map.get(key));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;6. 通过entrySet()返回的Set的iterator遍历&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;同上，map.entrySet()方法返回的是一个Set&amp;lt;Entry&amp;lt;String, String»类型的集合，可以使用iterator来访问该集合。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iterator&amp;lt;Entry&amp;lt;String, String&amp;gt;&amp;gt; it = 	map.entrySet().iterator();
while(it3.hasNext()) {
	Entry&amp;lt;String, String&amp;gt; entry = it3.next();
	System.out.println(entry.getKey() + &quot; : &quot; + 	entry.getValue());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上总结了对map集合的集中遍历方式，根据自身需要灵活选择使用哪种方式。&lt;/p&gt;
</description>
				<pubDate>Sun, 14 Jun 2015 00:00:00 +0800</pubDate>
				<link>/2015/06/14/Java%E4%B8%ADMap%E7%9A%84%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F.html</link>
				<guid isPermaLink="true">/2015/06/14/Java%E4%B8%ADMap%E7%9A%84%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F.html</guid>
			</item>
		
			<item>
				<title>spring io 平台介绍</title>
				<description>&lt;p&gt;Spring IO Platform reference对Spring IO的介绍如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Spring IO Platform is primarily intended to be used with a dependency management system. It works well with both Maven and Gradle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体如何理解Spring IO Platform 的作用了？&lt;/p&gt;

&lt;p&gt;以前在升级Spring项目的时候是手动的一个一个升级Spring模块的版本，并且一个模块与另一个模块之间的依赖适不适合你并不知道，你还需要测试或者找资料，所以比较麻烦。Spring IO Platform它能够结合Maven (或Gradle)管理每个模块的依赖，使得开发者不再花心思研究各个Java库相互依赖的版本，只需要引入Spring IO Platform即可，因为这些库的依赖关系Spring IO Platform已经帮你验证过了。&lt;/p&gt;

&lt;p&gt;在Maven中的使用也比较简单，只需要在pom.xml文件中加入依赖管理就可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependencyManagement&amp;gt;
	&amp;lt;dependencies&amp;gt;
		&amp;lt;dependency&amp;gt; 
			&amp;lt;groupId&amp;gt;io.spring.platform&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;platform-bom&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;1.1.2.BUILD-SNAPSHOT&amp;lt;/version&amp;gt; 
			&amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt;
			&amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt;
		&amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;
&amp;lt;/dependencyManagement&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也可以使用继承的方式：
	&lt;parent&gt; 
		&lt;groupid&gt;io.spring.platform&lt;/groupid&gt;
		&lt;artifactid&gt;platform-bom&lt;/artifactid&gt;
		&lt;version&gt;1.1.2.BUILD-SNAPSHOT&lt;/version&gt; 
		&lt;relativepath&gt;&lt;/relativepath&gt;
	&lt;/parent&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;p&gt;http://docs.spring.io/platform/docs/1.1.2.BUILD-SNAPSHOT/reference/htmlsingle/&lt;/p&gt;

</description>
				<pubDate>Fri, 16 Jan 2015 00:00:00 +0800</pubDate>
				<link>/2015/01/16/spring_io_platform.html</link>
				<guid isPermaLink="true">/2015/01/16/spring_io_platform.html</guid>
			</item>
		
			<item>
				<title>feedback_me 插件的使用</title>
				<description>&lt;p&gt;This jQuery plug-in allows user to easily add an animatable UI widget with a feedback form which slides from the side of the screen.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;feedback_me 在使用后的效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/5568742/5056362/a681f7e6-6cba-11e4-9eb2-bdcb2bf2d18d.png&quot; alt=&quot;1&quot; /&gt;
&lt;img src=&quot;https://cloud.githubusercontent.com/assets/5568742/5056363/c164806a-6cba-11e4-9f83-f38c9e18653a.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;它会附着在窗口的一边，点击后会出现信息表，填写完成后点击提交即可，提交完成后feedbach_me 会将填写的信息通过post请求发送到你指定的url地址，你可以在后台写一个controller专门负责接收这个请求。这个插件的好处在于整个过程使用简单、有效，只需要通过简单的配置就能完成。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;官方的文档的地址： http://plugins.jquery.com/feedback_me 
github的地址： https://github.com/vedmack/feedback_me&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;section&quot;&gt;看一下如何使用它：&lt;/h5&gt;

&lt;p&gt;首先他是一个jQuery 插件，需要在一开始引入jQuery，然后引入它指定的js和css文件即可。&lt;/p&gt;

&lt;p&gt;上图例子实现的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;link href=&quot;../../styles/jquery/jquery.feedback_me.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&amp;gt;
&amp;lt;script type=&quot;text/javascript&quot; src=&quot;../../scripts/jquery/jquery.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script type=&quot;text/javascript&quot; src=&quot;../../scripts/jquery/plugin/jquery.feedback_me.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
	$(document).ready(function(){
		fm_options = {
			// 这个参数是设置这个反馈按钮在页面的哪个位置，left-bottom ...
    		position: &quot;right-top&quot;,
    		// 是否显示邮箱
    		show_email: true,
    		email_label: &quot;邮箱&quot;,
    		email_required: true,
    		// 单选框
   			show_radio_button_list: true,
   			radio_button_list_labels: [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;],
   			radio_button_list_title: &quot;给个好评哦...&quot;,
   			radio_button_list_required: false,
   			//
   			name_required: true,
    		name_label: &quot;名字&quot;,
			// 
			message_label: &quot;反馈信息&quot;,
			message_required: true,
    		show_asterisk_for_required: true,
    		// 这个很重要，这是提交后信息发送到哪里
    		feedback_url: &quot;http://.../feedback/message&quot;,
    		// 提交信息后提示
    		delayed_options: {
        		send_fail : &quot;提交失败 :(. &quot;,
       			 send_success : &quot;提交成功，谢谢参与反馈，^_^ !&quot;
    		},
    		submit_label: &quot;提交&quot;,
			// 页面按钮提示信息
			trigger_label: &quot;意见反馈&quot;
		};
		//init feedback_me plugin
		fm.init(fm_options);
	});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Mon, 05 Jan 2015 00:00:00 +0800</pubDate>
				<link>/2015/01/05/feedback_me_%E6%8F%92%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8.html</link>
				<guid isPermaLink="true">/2015/01/05/feedback_me_%E6%8F%92%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8.html</guid>
			</item>
		
			<item>
				<title>Git提交错误后如何回退</title>
				<description>&lt;blockquote&gt;
  &lt;p&gt;在使用Git的时候需要维护一个自己的分支模型，推荐使用: 
http://nvie.com/posts/a-successful-git-branching-model/  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;section&quot;&gt;总体说来有一下两点：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;在中央仓库保存两个永久分支，master &amp;amp;&amp;amp; develop，master分支是发布分支，每次发布就是从master上打包发布，程序是不允许直接在master提交代码，只能通过其他分支合并到master分支。develop分支是开发分支，当develop上得源代码达到一个稳定状态的时候就可以把develop的代码合并到master上。&lt;/li&gt;
  &lt;li&gt;除去master和develop这两个永久分支，还存在一些暂时的分支。
    &lt;ul&gt;
      &lt;li&gt;线上的紧急Bug需要修复，这个时候就需要创建一个hotfix分支，这个分支是从master上检出，完成修复后双向合并到master和develop上，保证develop与master代码的同步，合并完后删除hotfix 分支。&lt;/li&gt;
      &lt;li&gt;还比如，要开发一个新功能，这个时候需要创建一个feature分支，这个分支就从develop上检出，可以把这个分支推到服务器上让更多地人参与开发，当然也可以不推倒服务器上，只在本地开发，开发完成后合并到develop上。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面这个是我们项目中的网络提交图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/5568742/5021605/91bdfa3e-6b12-11e4-8b66-3926fb2c0a29.png&quot; alt=&quot;test2&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这只是简单介说了一下分支模型，具体的介绍可以参考上面给出的链接。如果在提交的过程中直接在master上做了修改，或者不小心把master合并到了develop分支上，如何回退？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;master&quot;&gt;直接在master分支上修改代码并提交如何回退&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果直接在master分支上做了修改并提交到了服务器，这种操作在上面所描述的分支模型中是严格禁止的，如果出现这种情况如何回退：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;在本地切到master分支下，使用 git chenkout -b  temp-branch，这样就从master分支上创建了一个新的临时分支，并且切到这个分支下。&lt;/li&gt;
    &lt;li&gt;git reset –hard commitpoint, 这个commitpoint代表你要回滚的提交点&lt;/li&gt;
    &lt;li&gt;git branch -D master , 这个操作是删除本地master分支&lt;/li&gt;
    &lt;li&gt;git push origin :master, 删除远程服务器上得master分支，这里的删除就是推送一个空分支到远程master上。&lt;b&gt;但是注意的是远程master可能是一个default设置，这样服务器是不允许删除master分支，这个时候就需要在项目设置上将default标签切换到另一个分支上，上面的删除操作才能成功&lt;/b&gt;&lt;/li&gt;
    &lt;li&gt;将远程分支删掉后需要把回滚后的分支推到远程服务器上，git push origin temp-branch:master，这样就能完成回退操作&lt;/li&gt;
    &lt;li&gt;最后是删除temp-branch 分支，git checkout master, git branch -D temp-branch.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;
</description>
				<pubDate>Mon, 05 Jan 2015 00:00:00 +0800</pubDate>
				<link>/2015/01/05/Git%E6%8F%90%E4%BA%A4%E9%94%99%E8%AF%AF%E5%90%8E%E5%A6%82%E4%BD%95%E5%9B%9E%E9%80%80.html</link>
				<guid isPermaLink="true">/2015/01/05/Git%E6%8F%90%E4%BA%A4%E9%94%99%E8%AF%AF%E5%90%8E%E5%A6%82%E4%BD%95%E5%9B%9E%E9%80%80.html</guid>
			</item>
		
	</channel>
</rss>
