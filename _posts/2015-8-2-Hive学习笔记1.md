---
layout: post
title: Hive学习笔记1

---

#### 第一部分

__什么是Hive：__

Hive是基于Hadoop之上的数据仓库，数据存放在HDFS上，它同样可以通过ETL来进行数据的抽取、转换和加载。同时Hive可以自己开发Mapreduce程序来完成本身不能提供的数据处理操作。Hive本身就是一个SQL的解析引擎，他将SQL	语句转成Mapreduce任务在hadoop之上执行。

__什么是数据仓库：__

数据仓库是一个面向主题的，集成的，不可更新的，随时间不变化的数据集合，它用于支持企业或组织的决策分析处理。

	针对数据仓库的概念的解释：首先数据仓库中的数据是面向主题的，也就是这些数据的都是为了描述同一类事情，同时它的数据主要用于查询操作，不会对数据仓库中的数据进行删除和更新操作。

__OLTP:__ 联机事务处理（面向的是事务，需要实时的更新操作，银行转账）<br>
__OLAP:__ 联机分析处理（面向历史数据，进行数据的分析与挖掘，主要面向查询，不会做更新和插入数据，推荐系统）

__注意:__<br>

在搭建数据仓库的过程最常用的两种模型就是：星型模型和雪花模型，雪花模型是在星型模型上发展出来的。什么是星型模型，比如一个商品的推荐系统，主题应该是商品，但是围绕商品的有客户信息、厂家信息、促销信息等很多信息，这样就组成了一个星型模型。但是客户信息中也存在客户的家庭的信息、地址信息等。这样再关联的话就是一个雪花模型了。

---

#### 第二部分

__Hive的体系结构：__

Hive将元数据存储在数据库中（metastore），这个数据库支持mysql、derby等数据库中。Hive默认是存储在derby数据库中。

Hive的元数据有哪些？ 包括表的名字、表的列和分区及其属性，表的属性包括是否为外部表等，表的数据所在目录等。

首先Hive是基于Hadoop的，所以hive的数据会使用HDFS进行保存，同时hive的查询操作也是转化成hadoop的MapReduce操作，所以在hive中会存在一个Hive Driver：包括编译器、解析器和优化器。

在Hive的驱动之前有访问接口、jdbc以及WebConsole等方式进行操作。当然hive的元信息是存放在关系型数据库中的。

__HQL的执行过程：__

解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后的Mapreduce调用执行。

---

#### 第三部分

__Hive的安装：__

Hive的官网地址： hive.apache.org
apache 的历史工程发布页面在： archive.apache.org，在这个页面下可以找到hive工程。这里使用的是0.13版本。

Hive的安装之前需要先将hadoop安装好，hive有三种安装方式：嵌入模式、本地模式、远程模式。

__嵌入模式：__<br>
	Hive将元信息存储在Hive自带的Derby数据库中。但是这种操作方式存在一些缺陷：
	1. 只允许创建一个连接，也就是只允许一个用户操作hive
	2. 多用于Demo

在安装hive之前需要先安装hadoop，然后把hive的压缩包解压，在bin目录下执行./hive进入hive的启动脚本，这种方式就是使用的嵌入式模式启动hive，会在当前目录下生成一个metastore_db的目录，这个就是元信息目录。

__本地模式：__<br>
	Hive将元信息存储在mysql数据库中，mysql数据库与hive运行在同一台物理机上。这种方式可以允许多个用户操作hive，可以用于开发和测试。
	
__远程模式：（推荐使用这种方式）__<br>
	hive将元信息存储在mysql数据库中，mysql数据库与hive运行在不同的物理机上。

元信息存储在远程的mysql中。进入远程mysql中，mysql -u ... -p ... , 进入后使用create database hive创建一个hive数据库来保存元数据。
	
在hive解压后，把mysql的驱动jar包上传到hive的lib目录中。这样hive才能操作mysql数据库。然后需要更改hive的配置文件。在conf中创建一个hive-site.xml文件，文件的内容如下：
	
	<configuration>
		<property>
			<name>javax.jdo.option.ConnectionURL</name>
			<value>jdbc:mysql://ip:3306/hive</value>
			<name>javax.jdo.option.ConnectionDriverName</name>
			<value>com.mysql.jdbc.Driver</value>
			<name>javax.jdo.option.ConnectionUserName</name>
			<value>root</value>
			<name>javax.jdo.option.ConnectionPassword</name>
			<value>root</value>
		</property>
	</configuration>

当配置文件设置完后以后，就可以启动hive了。
	
---

#### 第四部分

__Hive的管理__

使用hive的脚本直接进入hive，或者 hive --service cli

cli常用的命令：

1. 清屏：ctrl + l 或者 !clear;
2. 查看数据仓库中的表： show tables;
3. 查看数据仓库中的函数：show functions;
4. 查看表结构： desc 表名；
5. 查看hdfs上的文件列表： dfs -ls /user 查看hadoop上/user上的文件
6. 执行linux命令： !命令
7. 执行HQL语句：select * from test;当执行这个语句的时候是不会开启一个MapReduce任务的，因为这个是获取全部的数据，获取全部的数据只需要把所有的数据读取出来就可以了，并不需要启动一个任务。但是在执行select name from test;查询某一个字段的信息的时候就需要启动一个MapReduce任务了。
8. 可以执行一个sql脚本：source /root/test.sql 这个语句就是启动一个执行sql脚本。这种方式就是和mysql执行外部的信息一样。
9. hive -S 进入hive启动任务不会产生调试信息，直接产生MapReduce的结果。
10. hive -e 执行sql语句。hive -e 'show tables';这样的执行就直接在linux的命令行操作就行，并不需要进入hive的交互式中执行。

__Web界面方式：__

启动方式： #hive --service hwi &
在0.13.0中并没有包含web管理的war包，需要自己编译。

下载hive源码包，并且解压源码包，然后进入源码路径下的hwi目录，使用：jar cvfM0 hive-hwi-0.13.0.war -C web/ . 这样就会打成一个war包，把这个war包拷贝到hive的lib目录下，同时需要修改hive-site.xml 配置文件，这个修改可以在wiki上看到。

	<property>
	  <name>hive.hwi.listen.host</name>
	  <value>0.0.0.0</value>
	  <description>This is the host address the Hive Web Interface will listen on</description>
	</property>
	 
	<property>
	  <name>hive.hwi.listen.port</name>
	  <value>9999</value>
	  <description>This is the port the Hive Web Interface will listen on</description>
	</property>
	 
	<property>
	  <name>hive.hwi.war.file</name>
	  <value>lib/hive-hwi-0.13.0.war</value>
	  <description>This is the WAR file with the jsp content for Hive Web Interface</description>
	</property>

这个就可以使用hive --service hwi启动web服务了，但是在访问这个web应用的时候，浏览器还是报出了500的错误。这个问题需要拷贝jdk的tools.jar 拷贝到hive的lib目录下。

这个时候就可以打开这个web界面了。

__hive的远程服务__

启动hive的远程服务的命令如下： hive --service hiveserver &
如果要使用jdbc连接hive进行操作，这个时候就需要开启hive的远程服务。

----

#### 第五部分

__Hive的数据类型__

* 基本数据类型：

	tinyint/smallint/int/bigint 整数类型<br>
	float/double 浮点类型<br>
	boolean 布尔类型<br>
	string/varchar/char 字符串类型<br>

* 复杂数据类型：

	array:数组类型，由一系列相同的数据类型的元素组成<br>
	map:集合类型，包含key->value键值对，可以通过key来访问元素。<br>
	struct:结构类型，可以包含不同数据类型的元素，这些元素可以通
	过“点语法”的方式来得到所需要的元素。<br>

---

	create table student (
	age int,
	name string,
	grade array<float>);
	
	插入的时候就是： {1, wangmin, [10,20,30]};
	
	create table student1 (
	sid int,
	sname string,
	grade map<string, float>);
	
	插入数据的时候： {1,wangmin,<'大学语文', 85>}
	
	create table studnet3(
	sid int,
	sname string,
	grades array<map<string, float>>);
	
	插入数据的时候：{1, wangmin, [<'大学语文‘， 12>,<'大学英语’，23>]}
	
	create table student4(
	sid int,
	info struct<nname:string, age:int, sex:string>);
	
	插入数据的时候：{1, {'wangmin', 23, ‘男’}}
	
* 时间类型:

	Date: 日期（年月日）<br>
	Timestamp: 是unix的一个时间偏移量<br>
	select unix_timestamp(); 查看系统的时间偏移量

---

#### 第六部分

__Hive的数据存储__

基于HDFS，没有专门的数据存储格式

Hive的数据模型：

1. 表可以分成以下几种：
2. Table 内部表
3. Partition 分区表
4. External Table 外部表
5. Bucket Table 桶表

---

	create table t1
	(tid int, tname string, age int);

	create table t2
	(tid int, tname string age int);
	localtion '/mytable/hive/t2';

	create table t3
	（tid int, tname string, age int)
	row format delimited fields terminated by ',';

	create table t4
	as
	select * from sample_date;

	create table t5
	row format delimited fields terminated by ','
	as
	select * from sample_data;

	分区表：

	create table partition_table
	(sid int, sname string)
	partitioned by (gender string)
	row format delimited fields terminated by ',';


创建这张表的时候就是以gender进行分区
分区表能够加快查询效率
