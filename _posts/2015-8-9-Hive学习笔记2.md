---
layout: post
title: Hive学习笔记2

---

#### Hive学习2

---

Hive 的数据导入： 可以使用Load命令导入，也可以使用Sqoop组件导入数据。

Hive不支持insert插入单条语句

Hive的数据查询 

Hive的Java客户端和自定义函数

---

Hive执行load导入数据。

load data local inpath '/root/data/student01.txt' into table t2;

local 表示数据从本地操作系统中导入。如果不写local表示从hdfs中导入。

local data local inpath '/root/data/student02.txt' into table t3;

将/root/data 下的所有数据导入t3表中，并且覆盖原来的数据
load data local inpath '/root/data/' overwrite into table t3

将HDFS中，/input/student01.txt导入到t3
load data inpath '/input/student01.txt' overwrite into table t3;

load data local inpath '/root/data/data1.txt' into table partition_table partition (gender='M');

Sqoop 数据的导入和导出

关于sqoop的导入和导出很简单，直接下载压缩包，然后解压，设置两个环境变量就可以。

设置第一个环境变量：

1. export HADOOP_COMMON_HOME=....(hadoop的安装目录）
2. export HADOOP_MAPRED_HOME=....(MapReduce的目录，这个目录和hadoop目录一般一样)

---

* 使用Sqoop导入Oracle数据到HDFS中

./sqoop import --connect jdbc:oracle:thin:@192.168.56.101:1521:orcl --username scott --password tiger --table emp --columns 'empno, ename, job, sal, deptno' -m 1 --target-dir '/sqoop/emp'

但是这样指向会报错，因为需要jdbc驱动，需要把oracle的jdbc传入到sqoop的lib目录下。

这样就会成功执行sqoop的数据导入和导出，整个执行过程就是通过MapReduce来完成的，

* 使用sqoop导入oracle数据到Hive中

./sqoop import --hive-import --connect jdbc:oracle:thin:@192.168.56.101:1521:orcl --username scott --password tiger --table emp -m 1 --columns 'empno,ename,job,sal,deptno'

* 使用sqoop导入oracle数据到Hive中，并且指定表名

./sqoop import --hive-import --connect jdbc:oracle:thin:@192.168.56.101:1521:orcl --username scott --password tiger --table emp -m 1 --columns 'empno,ename,job,sal,deptno' --hive-table emp1

* 使用sqoop导入oracle数据到hive表中，并使用where条件

./sqoop import --hive-import --connect jdbc:oracle:thin:@192.168.56.101:1521:orcl --username scott --password tiger --table emp -m 1 --columns 'empno,ename,job,sal,deptno' --hive-table emp1 --where 'DEPTNO=10'

* 使用sqoop导入oracle数据到hive中，并使用查询语句

./sqoop import --hive-import --connect jdbc:oracle:thin:@192.168.56.101:1521:orcl --username scott --password tiger -m 1 --query 'SELECT * FROM EMP WHERE SAL<2000 AND $CONDITIONS' --target-dir '/sqoop/emp5' --hive-table emp5

这里需要注意的是$CONDITIONS

* 使用sqoop将hive中的数据导出到oracle数据库中。

./sqoop export --connect jdbc:oracle:thin:@192.18/56.101:1521:orcl --username soctt --password tiger -m 1 --table MYEMP --export-dir .....

----

Hive的数据查询

在hive中进行数据操作的时候，总是将查询转换成一个MapReduce任务进行执行，但是对于简单的任务是没有必要转成一个MapReduce任务执行的，这个时候就可以配置一个fetch task功能，让fetch task功能支持简单的数据的操作。

配置方式有以下几种：

* 在hive的命令提示符下面执行：set hive.fetch.task.conversion=more;
* 在启动hive命令提示符的时候，加一个参数也可以开启这个功能：hive --hiveconf hive.fetch.task.conversion=more
* 修改hive-site.xml文件

前两种方式都可以配置这个功能，但是这种配置都是临时的。可以修改hive-site.xml文件使其永久有效。

修改hive-site.xml文件，加一行属性：

	<property>
		<name>hive.fetch.task.conversion</name>
		<value>more</value>
	</property>

----

Hive的函数：

* 内置函数
* 自定义函数

数学函数：

* round 四舍五入
* ceil 向上取整
* floor 向下取整

select round(45.926, 2); 45.926保留两位小数，最终结果是45.93

字符函数：

* lower
* upper
* length
* concat
* substr
* trim
* lpad
* rpad

select lower('Hello WORLD'), upper('hhhh')
select length('HelloWorld')
....

lpad:左填充
rpad:右填充

select lpad('abcd', 10, '*'), rpad('abcd', 10, '*')

收集函数

* size

size(map(<key, value>, <key, value>));

转换函数

* cast

select cast(1 as bigint);
select cast('2015-08-10' as date);

日期函数

* to_date
* year
* month
* day
* weekofyear
* datediff
* date_add
* date_sub

条件函数

* coalesce：从左到右返回第一个不为null的值
* case ... when ... : 条件表达式

聚合函数：

* count
* sum
* min
* max
* avg

表生成函数

* explode

---

Hive的表连接

* 等值连接
* 不等值连接
* 外连接
* 自连接

等值连接：

select e.empno, e.ename, e.sal, e.dname from emp e, dept d where e.deptno=d.deptno;

不等值连接：

外连接：

通过外连接可以将对于连接条件不成立的记录任然包含在最后的结果中：其中分成：

* 左外连接
* 右外连接

----

Hive的子查询：

---

Hive的java客户端操作：

首先需要启动Hive远程服务：hive --service hiveserver

* JDBC
* Thrift Client

Hive的JDBC客户端操作

1. 获取连接
2. 创建运行环境
3. 执行HQL
4. 处理结果
5. 释放资源

hive-jdbc-...jar 这个就是hive的jdbc文件。