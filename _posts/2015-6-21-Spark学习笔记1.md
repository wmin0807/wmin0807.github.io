---
layout: post
title: Spark学习笔记1

---

__Apache Spark is a fast and general-purpose cluster computing system.__

1. spark 提供了 Java  Scala Python and 的API。
在examples/src/main目录下有Java和Scala例子， 用 bin/run-example 运行。

2. 通过运行： ./bin/spark-shell --master local[2] 来进行交互式的操作，这是学习sprak最好的方式

3. 从1.4起spark也提供了R的api，./bin/sparkR --master local[2]

__Quick Start__

1. spark 提供了Scala 和 Python的交互式分析shell来学习api
./bin/spark-shell

2. RDD是分布式弹性数据集，可以理解为就是一个分布式的集合，这个集合的创建可以通过Hadoop 的 InputFormats，或者通过其他RDD转换而来。

3. RDD有动作，他能够返回值，以及转换操作，他会返回一个指针指向新的RDD，通过RDD的filter操作返回一个新的RDD.

4. RDD有很复杂的操作，他可以直接调用Scala Java的库方法，使用的时候需要使用：import java.lang.Math来引入。

__Caching__

1. spark 支持推送一个数据集到一个集群的缓存中，尤其是那种需要经常重复读取的数据。eg: linesWithSpark.cache()

__Self-contained Applications（独立的应用程序）__

* 创建Maven项目。
* pom.xml 文件如下：

----

    <dependencies>
       <dependency>
	      <groupId>org.apache.spark</groupId>
	      <artifactId>spark-core_2.10</artifactId>
	      <version>1.4.0</version>
	    </dependency>
	    <dependency>
	      <groupId>junit</groupId>
	      <artifactId>junit</artifactId>
	      <version>4.11</version>
	      <scope>test</scope>
	    </dependency>
	</dependencies>

  	
* 创建一个简单的spark程序：

----

	public class SimpleApp {
		public static void main(String[] args) {
			// 文件路径
			String logFile = "/home/wm/apps/spark-1.4.0-bin-hadoop2.6/README.md";
			SparkConf conf = new SparkConf().setAppName("Simple Application");
			JavaSparkContext sc = new JavaSparkContext(conf);
			JavaRDD<String> logData = sc.textFile(logFile).cache();
			@SuppressWarnings("serial")
			long numAs = logData.filter(new Function<String, Boolean>() {
				public Boolean call(String s) throws Exception {
					return s.contains("a");
				}
				
			}).count();
			@SuppressWarnings("serial")
			long numBs = logData.filter(new Function<String, Boolean>() {

				public Boolean call(String s) throws Exception {
					return s.contains("b");
				}
				
			}).count();
			System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);
			sc.close();
		}
	}

* mvn pckage
* 将target目录下的sparkdemo2-0.0.1-SNAPSHOT.jar文件放在spark的安装目录下。
* 在spark安装目录下执行jar包中的程序，但是要指定执行的class文件，这个class文件需要全路径。例如：

----

    ./bin/spark-submit --class "com.wm.test.sparkdemo" --master local[4] sparkdemo-0.0.1-SNAPSHOT.jar

